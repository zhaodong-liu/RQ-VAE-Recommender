import torch

from data.schemas import TokenizedSeqBatch
from torch import nn
from torch import Tensor
from typing import NamedTuple


class SemIdEmbeddingBatch(NamedTuple):
    seq: Tensor
    fut: Tensor


class SemIdEmbedder(nn.Module):
    def __init__(self, num_embeddings, sem_ids_dim, embeddings_dim) -> None:
        super().__init__()
        
        self.sem_ids_dim = sem_ids_dim
        self.num_embeddings = num_embeddings
        self.padding_idx = sem_ids_dim*num_embeddings
        
        self.emb = nn.Embedding(
            num_embeddings=num_embeddings*self.sem_ids_dim+1,
            embedding_dim=embeddings_dim,
            padding_idx=self.padding_idx
        )
    
    # def forward(self, batch: TokenizedSeqBatch) -> SemIdEmbeddingBatch:
    #     # ç¡®ä¿ token_type_ids åœ¨æœ‰æ•ˆèŒƒå›´å†…
    #     token_type_ids_clamped = torch.clamp(batch.token_type_ids, 0, self.sem_ids_dim - 1)
        
    #     # è®¡ç®—å®žé™…çš„embeddingç´¢å¼•
    #     sem_ids = token_type_ids_clamped * self.num_embeddings + batch.sem_ids
        
    #     # æ£€æŸ¥å¹¶ä¿®å¤è¶Šç•Œçš„ç´¢å¼•
    #     max_valid_idx = self.num_embeddings * self.sem_ids_dim - 1
        
    #     # å°†è¶…å‡ºèŒƒå›´çš„ç´¢å¼•è®¾ä¸ºpadding_idx
    #     sem_ids = torch.where(
    #         (batch.sem_ids >= 0) & (batch.sem_ids < self.num_embeddings) & 
    #         (token_type_ids_clamped >= 0) & (token_type_ids_clamped < self.sem_ids_dim),
    #         sem_ids,
    #         self.padding_idx
    #     )
        
    #     # å¯¹äºŽæ— æ•ˆä½ç½®ï¼Œä½¿ç”¨padding_idx
    #     sem_ids[~batch.seq_mask] = self.padding_idx

    #     if batch.sem_ids_fut is not None:
    #         # åŒæ ·å¤„ç†future token_type_ids
    #         token_type_ids_fut_clamped = torch.clamp(batch.token_type_ids_fut, 0, self.sem_ids_dim - 1)
    #         sem_ids_fut = token_type_ids_fut_clamped * self.num_embeddings + batch.sem_ids_fut
            
    #         # åŒæ ·æ£€æŸ¥futureè¯­ä¹‰IDçš„è¾¹ç•Œ
    #         sem_ids_fut = torch.where(
    #             (batch.sem_ids_fut >= 0) & (batch.sem_ids_fut < self.num_embeddings) &
    #             (token_type_ids_fut_clamped >= 0) & (token_type_ids_fut_clamped < self.sem_ids_dim),
    #             sem_ids_fut,
    #             self.padding_idx
    #         )
            
    #         sem_ids_fut = self.emb(sem_ids_fut)
    #     else:
    #         sem_ids_fut = None
            
    #     return SemIdEmbeddingBatch(
    #         seq=self.emb(sem_ids),
    #         fut=sem_ids_fut
    #     ) 

    def forward(self, batch: TokenizedSeqBatch) -> SemIdEmbeddingBatch:
        print(f"\nðŸ” SemIdEmbedderè°ƒè¯•:")
        print(f"  self.num_embeddings: {self.num_embeddings}")
        print(f"  self.sem_ids_dim: {self.sem_ids_dim}")
        print(f"  self.padding_idx: {self.padding_idx}")
        
        # è¯¦ç»†æ£€æŸ¥æ¯ä¸ªè¾“å…¥tensor
        print(f"  è¾“å…¥æ£€æŸ¥:")
        print(f"    sem_ids: shape={batch.sem_ids.shape}, range=[{batch.sem_ids.min()}, {batch.sem_ids.max()}]")
        print(f"    token_type_ids: shape={batch.token_type_ids.shape}, range=[{batch.token_type_ids.min()}, {batch.token_type_ids.max()}]")
        
        # æ£€æŸ¥è¾¹ç•Œæ¡ä»¶
        invalid_sem_ids = (batch.sem_ids >= self.num_embeddings) & (batch.sem_ids != -1)
        if invalid_sem_ids.any():
            print(f"    âŒ å‘çŽ°æ— æ•ˆsem_ids: {batch.sem_ids[invalid_sem_ids].unique()}")
            
        invalid_token_type = batch.token_type_ids >= self.sem_ids_dim
        if invalid_token_type.any():
            print(f"    âŒ å‘çŽ°æ— æ•ˆtoken_type_ids: {batch.token_type_ids[invalid_token_type].unique()}")

        # å¤¹å–è¾¹ç•Œ
        token_type_ids_clamped = torch.clamp(batch.token_type_ids, 0, self.sem_ids_dim - 1)
        
        # è®¡ç®—embeddingç´¢å¼•
        sem_ids = token_type_ids_clamped * self.num_embeddings + batch.sem_ids
        
        # æ£€æŸ¥è®¡ç®—åŽçš„ç´¢å¼•
        max_valid_idx = self.num_embeddings * self.sem_ids_dim - 1
        print(f"  è®¡ç®—åŽçš„ç´¢å¼•:")
        print(f"    sem_ids range: [{sem_ids.min()}, {sem_ids.max()}]")
        print(f"    max_valid_idx: {max_valid_idx}")
        
        # æ£€æŸ¥æœ€ç»ˆç´¢å¼•æ˜¯å¦è¶Šç•Œ
        final_invalid = (sem_ids > max_valid_idx) | (sem_ids < 0)
        final_invalid = final_invalid & (batch.sem_ids != -1)  # æŽ’é™¤padding
        
        if final_invalid.any():
            print(f"    âŒ æœ€ç»ˆç´¢å¼•è¶Šç•Œ: {sem_ids[final_invalid].unique()}")
            sem_ids = torch.where(final_invalid, self.padding_idx, sem_ids)
            print(f"    ðŸ”§ å·²ä¿®å¤ä¸ºpadding_idx")

        # å¤„ç†seq_mask
        if batch.seq_mask is not None:
            sem_ids[~batch.seq_mask] = self.padding_idx

        # å¤„ç†futureæ•°æ®
        sem_ids_fut = None
        if batch.sem_ids_fut is not None:
            print(f"  å¤„ç†futureæ•°æ®:")
            print(f"    sem_ids_fut: shape={batch.sem_ids_fut.shape}, range=[{batch.sem_ids_fut.min()}, {batch.sem_ids_fut.max()}]")
            print(f"    token_type_ids_fut: shape={batch.token_type_ids_fut.shape}, range=[{batch.token_type_ids_fut.min()}, {batch.token_type_ids_fut.max()}]")
            
            # åŒæ ·çš„æ£€æŸ¥å’Œä¿®å¤æµç¨‹
            invalid_fut_sem = (batch.sem_ids_fut >= self.num_embeddings) & (batch.sem_ids_fut != -1)
            if invalid_fut_sem.any():
                print(f"    âŒ å‘çŽ°æ— æ•ˆsem_ids_fut: {batch.sem_ids_fut[invalid_fut_sem].unique()}")
                
            invalid_fut_type = batch.token_type_ids_fut >= self.sem_ids_dim
            if invalid_fut_type.any():
                print(f"    âŒ å‘çŽ°æ— æ•ˆtoken_type_ids_fut: {batch.token_type_ids_fut[invalid_fut_type].unique()}")

            token_type_ids_fut_clamped = torch.clamp(batch.token_type_ids_fut, 0, self.sem_ids_dim - 1)
            sem_ids_fut_computed = token_type_ids_fut_clamped * self.num_embeddings + batch.sem_ids_fut
            
            print(f"    è®¡ç®—åŽsem_ids_fut range: [{sem_ids_fut_computed.min()}, {sem_ids_fut_computed.max()}]")
            
            fut_final_invalid = (sem_ids_fut_computed > max_valid_idx) | (sem_ids_fut_computed < 0)
            fut_final_invalid = fut_final_invalid & (batch.sem_ids_fut != -1)
            
            if fut_final_invalid.any():
                print(f"    âŒ futureæœ€ç»ˆç´¢å¼•è¶Šç•Œ: {sem_ids_fut_computed[fut_final_invalid].unique()}")
                sem_ids_fut_computed = torch.where(fut_final_invalid, self.padding_idx, sem_ids_fut_computed)
                print(f"    ðŸ”§ å·²ä¿®å¤futureç´¢å¼•")
            
            try:
                sem_ids_fut = self.emb(sem_ids_fut_computed)
                print(f"    âœ… future embeddingæˆåŠŸ")
            except Exception as e:
                print(f"    âŒ future embeddingå¤±è´¥: {e}")
                raise e

        # ä¸»è¦çš„embeddingæŸ¥æ‰¾
        try:
            print(f"  æ‰§è¡Œä¸»embeddingæŸ¥æ‰¾...")
            seq_emb = self.emb(sem_ids)
            print(f"    âœ… ä¸»embeddingæˆåŠŸ, shape: {seq_emb.shape}")
        except Exception as e:
            print(f"    âŒ ä¸»embeddingå¤±è´¥: {e}")
            print(f"    sem_idsè¯¦ç»†ä¿¡æ¯: min={sem_ids.min()}, max={sem_ids.max()}, shape={sem_ids.shape}")
            print(f"    embeddingå±‚å¤§å°: {self.emb.num_embeddings}")
            raise e

        return SemIdEmbeddingBatch(seq=seq_emb, fut=sem_ids_fut)

    

class UserIdEmbedder(nn.Module):
    # TODO: Implement hashing trick embedding for user id
    def __init__(self, num_buckets, embedding_dim) -> None:
        super().__init__()
        self.num_buckets = num_buckets
        self.emb = nn.Embedding(num_buckets, embedding_dim)
    
    def forward(self, x: Tensor) -> Tensor:
        hashed_indices = x % self.num_buckets
        # hashed_indices = torch.tensor([hash(token) % self.num_buckets for token in x], device=x.device)
        return self.emb(hashed_indices)