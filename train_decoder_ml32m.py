import argparse
import os
import gin
import torch
import wandb
import traceback

# ============ CUDAè°ƒè¯•è®¾ç½® ============
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'
os.environ['TORCH_COMPILE_DISABLE'] = '1'
torch.autograd.set_detect_anomaly(True)

def check_cuda_error(operation_name=""):
    """æ£€æŸ¥CUDAé”™è¯¯å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯"""
    if torch.cuda.is_available():
        try:
            torch.cuda.synchronize()
        except RuntimeError as e:
            print(f"âŒ CUDAé”™è¯¯ in {operation_name}: {e}")
            raise

def force_to_device(obj, target_device, name=""):
    """å¼ºåˆ¶å°†å¼ é‡æˆ–åŒ…å«å¼ é‡çš„å¯¹è±¡é€’å½’åœ°ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡"""
    if torch.is_tensor(obj):
        if obj.device != target_device:
            print(f"ğŸ”§ ç§»åŠ¨ {name}: {obj.device} -> {target_device}")
            return obj.to(target_device)
        return obj
    elif hasattr(obj, '_asdict'):  # NamedTupleï¼ˆæ¯”å¦‚TokenizedSeqBatchï¼‰
        fields = obj._asdict()
        moved_fields = {
            k: force_to_device(v, target_device, f"{name}.{k}")
            for k, v in fields.items()
        }
        return type(obj)(**moved_fields)
    elif isinstance(obj, (list, tuple)):
        return type(obj)(force_to_device(x, target_device, f"{name}[{i}]") for i, x in enumerate(obj))
    elif isinstance(obj, dict):
        return {k: force_to_device(v, target_device, f"{name}.{k}") for k, v in obj.items()}
    elif hasattr(obj, '__dataclass_fields__'):  # dataclass
        from dataclasses import replace
        return replace(obj, **{
            k: force_to_device(v, target_device, f"{name}.{k}")
            for k, v in vars(obj).items()
        })
    else:
        return obj

def create_safe_tokenized_data(tokenized_data, operation_name="unknown"):
    """åˆ›å»ºå®‰å…¨çš„tokenizedæ•°æ®ï¼Œå½»åº•ä¿®å¤èŒƒå›´å’Œè®¾å¤‡é—®é¢˜"""
    verbose = "åˆå§‹æµ‹è¯•" in operation_name or "ç¬¬ä¸€æ¬¡" in operation_name or "iter0" in operation_name
    
    if verbose:
        print(f"ğŸ”§ å½»åº•ä¿®å¤ {operation_name} çš„æ‰€æœ‰é—®é¢˜...")
    
    # ğŸ”§ CRITICAL: å¼ºåˆ¶ç¡®å®šç›®æ ‡è®¾å¤‡
    target_device = None
    for attr_name in ['sem_ids', 'sem_ids_fut', 'user_ids', 'seq_mask', 'token_type_ids', 'token_type_ids_fut']:
        if hasattr(tokenized_data, attr_name):
            tensor = getattr(tokenized_data, attr_name)
            if torch.is_tensor(tensor) and tensor.is_cuda:
                target_device = tensor.device
                break
    
    if target_device is None:
        target_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    if verbose:
        print(f"  ğŸ¯ ç›®æ ‡è®¾å¤‡: {target_device}")
    
    # ğŸ”§ ç¬¬ä¸€æ­¥ï¼šå¼ºåˆ¶æ‰€æœ‰å¼ é‡åˆ°ç›®æ ‡è®¾å¤‡
    user_ids = force_to_device(tokenized_data.user_ids, target_device, "user_ids")
    sem_ids = force_to_device(tokenized_data.sem_ids, target_device, "sem_ids")
    sem_ids_fut = force_to_device(tokenized_data.sem_ids_fut, target_device, "sem_ids_fut")
    seq_mask = force_to_device(tokenized_data.seq_mask, target_device, "seq_mask")
    token_type_ids = force_to_device(tokenized_data.token_type_ids, target_device, "token_type_ids")
    token_type_ids_fut = force_to_device(tokenized_data.token_type_ids_fut, target_device, "token_type_ids_fut")
    
    # ğŸ”§ ç¬¬äºŒæ­¥ï¼šä¿®å¤è¯­ä¹‰IDèŒƒå›´ï¼ˆæ›´ä¸¥æ ¼çš„é’³åˆ¶ï¼‰
    if verbose:
        print(f"  åŸå§‹ sem_ids èŒƒå›´: [{sem_ids.min()}, {sem_ids.max()}]")
        print(f"  åŸå§‹ sem_ids_fut èŒƒå›´: [{sem_ids_fut.min()}, {sem_ids_fut.max()}]")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºå®‰å…¨èŒƒå›´çš„å€¼
    sem_ids_out_of_range = (sem_ids > 255) & (sem_ids != -1)
    sem_ids_fut_out_of_range = (sem_ids_fut > 255) & (sem_ids_fut != -1)
    
    if sem_ids_out_of_range.any():
        out_of_range_values = sem_ids[sem_ids_out_of_range].unique()
        print(f"  âš ï¸  å‘ç° {sem_ids_out_of_range.sum()} ä¸ªsem_idsè¶…å‡ºèŒƒå›´[0,255]: {out_of_range_values[:10].tolist()}")
    
    if sem_ids_fut_out_of_range.any():
        out_of_range_values = sem_ids_fut[sem_ids_fut_out_of_range].unique()
        print(f"  âš ï¸  å‘ç° {sem_ids_fut_out_of_range.sum()} ä¸ªsem_ids_futè¶…å‡ºèŒƒå›´[0,255]: {out_of_range_values[:10].tolist()}")
    
    # ğŸ”§ CRITICAL: ä¸¥æ ¼é’³åˆ¶åˆ°å®‰å…¨èŒƒå›´ [0, 255]ï¼Œä¿æŒ-1ä½œä¸ºpadding
    max_safe_sem_id = 255
    
    safe_sem_ids = torch.where(
        sem_ids == -1,
        torch.tensor(-1, device=target_device, dtype=sem_ids.dtype),
        torch.clamp(sem_ids, 0, max_safe_sem_id)
    )
    
    safe_sem_ids_fut = torch.where(
        sem_ids_fut == -1,
        torch.tensor(-1, device=target_device, dtype=sem_ids_fut.dtype),
        torch.clamp(sem_ids_fut, 0, max_safe_sem_id)
    )
    
    # éªŒè¯ä¿®å¤ç»“æœ
    if verbose:
        print(f"  ä¿®å¤å sem_ids èŒƒå›´: [{safe_sem_ids.min()}, {safe_sem_ids.max()}]")
        print(f"  ä¿®å¤å sem_ids_fut èŒƒå›´: [{safe_sem_ids_fut.min()}, {safe_sem_ids_fut.max()}]")
        
        # ç¡®ä¿æ²¡æœ‰è¶…å‡ºèŒƒå›´çš„å€¼
        assert safe_sem_ids.max() <= 255 or (safe_sem_ids == -1).all(), f"sem_idsä»æœ‰è¶…å‡ºèŒƒå›´çš„å€¼: {safe_sem_ids.max()}"
        assert safe_sem_ids_fut.max() <= 255 or (safe_sem_ids_fut == -1).all(), f"sem_ids_futä»æœ‰è¶…å‡ºèŒƒå›´çš„å€¼: {safe_sem_ids_fut.max()}"
        print(f"  âœ… éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰è¯­ä¹‰IDéƒ½åœ¨å®‰å…¨èŒƒå›´å†…")
    
    # ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šä¿®å¤user_idsèŒƒå›´ï¼ˆUserIdEmbedderä½¿ç”¨å–æ¨¡æ“ä½œï¼‰
    # UserIdEmbedderæœ‰2000ä¸ªbucketsï¼Œæ‰€ä»¥user_idsåº”è¯¥åœ¨[0, 1999]èŒƒå›´å†…
    # ä½†å®é™…ä¸ŠUserIdEmbedderä¼šè‡ªåŠ¨å–æ¨¡ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦ç¡®ä¿user_idsæ˜¯æœ‰æ•ˆçš„æ•´æ•°
    safe_user_ids = torch.clamp(user_ids, 0, 999999)  # ç¡®ä¿æ˜¯æ­£æ•´æ•°ï¼Œå–æ¨¡ä¼šå¤„ç†èŒƒå›´
    
    if verbose and not torch.equal(user_ids, safe_user_ids):
        print(f"  ä¿®å¤äº† user_ids èŒƒå›´: [{user_ids.min()}, {user_ids.max()}] -> [{safe_user_ids.min()}, {safe_user_ids.max()}]")
    
    # åˆ›å»ºæ–°çš„TokenizedSeqBatchå¯¹è±¡
    from data.schemas import TokenizedSeqBatch
    
    safe_tokenized_data = TokenizedSeqBatch(
        user_ids=safe_user_ids,
        sem_ids=safe_sem_ids,
        sem_ids_fut=safe_sem_ids_fut,
        seq_mask=seq_mask,
        token_type_ids=token_type_ids,
        token_type_ids_fut=token_type_ids_fut
    )
    
    if verbose:
        print(f"  âœ… æœ€ç»ˆéªŒè¯æ‰€æœ‰å¼ é‡è®¾å¤‡å’ŒèŒƒå›´:")
        print(f"    user_ids: {safe_tokenized_data.user_ids.device}, èŒƒå›´[{safe_tokenized_data.user_ids.min()}, {safe_tokenized_data.user_ids.max()}]")
        print(f"    sem_ids: {safe_tokenized_data.sem_ids.device}, èŒƒå›´[{safe_tokenized_data.sem_ids.min()}, {safe_tokenized_data.sem_ids.max()}]")
        print(f"    sem_ids_fut: {safe_tokenized_data.sem_ids_fut.device}, èŒƒå›´[{safe_tokenized_data.sem_ids_fut.min()}, {safe_tokenized_data.sem_ids_fut.max()}]")
        print(f"    seq_mask: {safe_tokenized_data.seq_mask.device}")
        print(f"    token_type_ids: {safe_tokenized_data.token_type_ids.device}")
        print(f"    token_type_ids_fut: {safe_tokenized_data.token_type_ids_fut.device}")
        
    return safe_tokenized_data

def safe_model_forward(model, tokenized_data, operation_name="forward"):
    """å®‰å…¨çš„æ¨¡å‹å‰å‘ä¼ æ’­ï¼Œå½»åº•è§£å†³è®¾å¤‡é—®é¢˜"""
    try:
        print(f"\nğŸš€ å¼€å§‹ {operation_name}...")
        
        # ğŸ”§ CRITICAL: å¼ºåˆ¶ä¿®å¤æ‰€æœ‰è®¾å¤‡å’ŒèŒƒå›´é—®é¢˜
        corrected_tokenized_data = create_safe_tokenized_data(tokenized_data, operation_name)
        
        # ğŸ”§ é¢å¤–éªŒè¯ï¼šç¡®ä¿æ¨¡å‹å’Œæ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Š
        model_device = next(model.parameters()).device
        corrected_tokenized_data = force_to_device(corrected_tokenized_data, model_device, "corrected_tokenized_data")

        # ğŸ›¡ï¸ æ£€æŸ¥æ‰€æœ‰å­—æ®µæ˜¯å¦ä»åœ¨ CPUï¼ˆé˜²æ­¢ Triton æŠ¥é”™ï¼‰
        for attr_name in corrected_tokenized_data.__annotations__:
            tensor = getattr(corrected_tokenized_data, attr_name, None)
            if torch.is_tensor(tensor) and not tensor.is_cuda:
                raise RuntimeError(f"[CRITICAL] {attr_name} æ˜¯ CPU tensor! device={tensor.device}, shape={tensor.shape}")

        # ğŸ”§ ä¿®å¤UserIdEmbedder.forwardçš„è®¾å¤‡ä¸ä¸€è‡´é—®é¢˜
        original_user_id_forward = model.user_id_embedder.forward

        def fixed_user_id_forward(x):
            device = x.device
            hashed_indices = (x.to(device) % model.user_id_embedder.num_buckets).to(device)
            return model.user_id_embedder.emb(hashed_indices)

        model.user_id_embedder.forward = fixed_user_id_forward

        try:
            check_cuda_error(f"before_{operation_name}")
            print(f"ğŸ“¡ è°ƒç”¨æ¨¡å‹...")

            output = model(corrected_tokenized_data)
            print(f"âœ… æ¨¡å‹è°ƒç”¨æˆåŠŸ")

            check_cuda_error(f"after_{operation_name}")
            return output

        finally:
            model.user_id_embedder.forward = original_user_id_forward

    except Exception as e:
        print(f"\nâŒ {operation_name} å¤±è´¥!")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")

        if "device" in str(e).lower() or "Expected all tensors to be on the same device" in str(e):
            print(f"\nğŸ“ è®¾å¤‡ä¸åŒ¹é…è¯¦ç»†åˆ†æ:")
            model_device = next(model.parameters()).device
            print(f"  æ¨¡å‹è®¾å¤‡: {model_device}")
            print(f"  Embeddingå±‚è®¾å¤‡:")
            print(f"    user_id_embedder.emb: {model.user_id_embedder.emb.weight.device}")
            print(f"    sem_id_embedder.emb: {model.sem_id_embedder.emb.weight.device}")
            print(f"  è¾“å…¥æ•°æ®è®¾å¤‡:")
            for attr_name in corrected_tokenized_data.__annotations__:
                tensor = getattr(corrected_tokenized_data, attr_name, None)
                if torch.is_tensor(tensor):
                    print(f"    {attr_name}: {tensor.device}, å½¢çŠ¶: {tensor.shape}")

        if "index" in str(e).lower() or "out of range" in str(e).lower():
            print(f"\nğŸ“ ç´¢å¼•èŒƒå›´è¯¦ç»†åˆ†æ:")
            for attr_name in ['user_ids', 'sem_ids', 'sem_ids_fut']:
                if hasattr(corrected_tokenized_data, attr_name):
                    tensor = getattr(corrected_tokenized_data, attr_name)
                    if torch.is_tensor(tensor):
                        unique_vals = torch.unique(tensor)
                        print(f"  {attr_name}: èŒƒå›´[{tensor.min()}, {tensor.max()}], å”¯ä¸€å€¼æ•°={unique_vals.numel()}")
                        if unique_vals.numel() <= 20:
                            print(f"    æ‰€æœ‰å”¯ä¸€å€¼: {unique_vals.tolist()}")
                        else:
                            print(f"    å‰10ä¸ª: {unique_vals[:10].tolist()}")
                            print(f"    å10ä¸ª: {unique_vals[-10:].tolist()}")

        print(f"\nğŸ“š å®Œæ•´é”™è¯¯å †æ ˆ:")
        traceback.print_exc()
        raise

# ============ ä¸»è¦è®­ç»ƒä»£ç  ============
from accelerate import Accelerator
from data.processed import ItemData, RecDataset, SeqData
from data.utils import batch_to, cycle, next_batch
from evaluate.metrics import TopKAccumulator
from modules.model import EncoderDecoderRetrievalModel
from modules.scheduler.inv_sqrt import InverseSquareRootScheduler
from modules.tokenizer.semids import SemanticIdTokenizer
from modules.utils import compute_debug_metrics, parse_config
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm

@gin.configurable
def train(
    iterations=500,    # æ›´å°‘è¿­ä»£ç”¨äºå¿«é€ŸéªŒè¯
    batch_size=4,      # æ›´å°batch sizeä»¥å‡å°‘å¤æ‚åº¦
    learning_rate=0.001,
    weight_decay=0.01,
    dataset_folder="dataset/ml-32m",
    save_dir_root="out/",
    dataset=RecDataset.ML_32M,
    pretrained_rqvae_path=None,
    pretrained_decoder_path=None,
    split_batches=True,
    amp=False,
    wandb_logging=False,
    force_dataset_process=False,
    mixed_precision_type="fp16",
    gradient_accumulate_every=1,
    save_model_every=1000000,
    partial_eval_every=100,
    full_eval_every=10000,  # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„å‚æ•°
    vae_input_dim=768,
    vae_embed_dim=64,
    vae_hidden_dims=[512, 256, 128],
    vae_codebook_size=256,
    vae_codebook_normalize=False,
    vae_sim_vq=False,
    vae_n_cat_feats=18,
    vae_n_layers=3,
    decoder_embed_dim=64,
    dropout_p=0.1,
    attn_heads=8,
    attn_embed_dim=64,
    attn_layers=4,
    push_vae_to_hf=False,  # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„å‚æ•°
    train_data_subsample=True,
    model_jagged_mode=True,
    vae_hf_model_name="edobotta/rqvae-movielens32m",  # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„å‚æ•°
):
    print("ğŸš€ å¼€å§‹è®­ç»ƒ - æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ï¼ˆè®¾å¤‡+ç´¢å¼•ï¼‰")
    
    accelerator = Accelerator(split_batches=split_batches, mixed_precision='no')
    device = accelerator.device
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ•°æ®é›†
    try:
        print("ğŸ“š åŠ è½½æ•°æ®é›†...")
        item_dataset = ItemData(root=dataset_folder, dataset=dataset, force_process=force_dataset_process)
        train_dataset = SeqData(root=dataset_folder, dataset=dataset, is_train=True, subsample=train_data_subsample)
        eval_dataset = SeqData(root=dataset_folder, dataset=dataset, is_train=False, subsample=False)
        
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: ç‰©å“={len(item_dataset)}, è®­ç»ƒ={len(train_dataset)}, è¯„ä¼°={len(eval_dataset)}")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        raise

    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    train_dataloader = cycle(train_dataloader)
    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)
    
    train_dataloader, eval_dataloader = accelerator.prepare(train_dataloader, eval_dataloader)

    # åˆå§‹åŒ–tokenizer
    try:
        print("ğŸ”¤ åˆå§‹åŒ–tokenizer...")
        tokenizer = SemanticIdTokenizer(
            input_dim=vae_input_dim, hidden_dims=vae_hidden_dims, output_dim=vae_embed_dim,
            codebook_size=vae_codebook_size, n_layers=vae_n_layers, n_cat_feats=vae_n_cat_feats,
            rqvae_weights_path=pretrained_rqvae_path, rqvae_codebook_normalize=vae_codebook_normalize,
            rqvae_sim_vq=vae_sim_vq
        )
        tokenizer = accelerator.prepare(tokenizer)
        
        print("ğŸ—ï¸ é¢„è®¡ç®—è¯­æ–™åº“IDs...")
        tokenizer.precompute_corpus_ids(item_dataset)
        print(f"âœ… Tokenizeråˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Tokenizeråˆå§‹åŒ–å¤±è´¥: {e}")
        raise

    # åˆå§‹åŒ–æ¨¡å‹
    try:
        print("ğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
        model_num_embeddings = 1091
        
        model = EncoderDecoderRetrievalModel(
            embedding_dim=decoder_embed_dim, attn_dim=attn_embed_dim, dropout=dropout_p,
            num_heads=attn_heads, n_layers=attn_layers, num_embeddings=model_num_embeddings,
            inference_verifier_fn=lambda x: tokenizer.exists_prefix(x), sem_id_dim=tokenizer.sem_ids_dim,
            max_pos=train_dataset.max_seq_len * tokenizer.sem_ids_dim + 256, jagged_mode=model_jagged_mode
        )
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥embeddingå±‚è®¾å¤‡
        print(f"ğŸ“ Embeddingå±‚è®¾å¤‡æ£€æŸ¥:")
        print(f"  user_id_embedder.emb: {model.user_id_embedder.emb.weight.device}")
        print(f"  sem_id_embedder.emb: {model.sem_id_embedder.emb.weight.device}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

    # ğŸ”§ CRITICAL: æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
    try:
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        model.eval()
        model.to(device)
        
        test_batch = next_batch(train_dataloader, device)
        test_tokenized = tokenizer(test_batch)

        test_tokenized = force_to_device(test_tokenized, device, name="test_tokenized")
        
        print(f"ğŸ” æµ‹è¯•æ•°æ®åŸå§‹çŠ¶æ€:")
        print(f"  user_ids: {test_tokenized.user_ids.device}, èŒƒå›´[{test_tokenized.user_ids.min()}, {test_tokenized.user_ids.max()}]")
        print(f"  sem_ids: {test_tokenized.sem_ids.device}, èŒƒå›´[{test_tokenized.sem_ids.min()}, {test_tokenized.sem_ids.max()}]")
        print(f"  sem_ids_fut: {test_tokenized.sem_ids_fut.device}, èŒƒå›´[{test_tokenized.sem_ids_fut.min()}, {test_tokenized.sem_ids_fut.max()}]")
        
        with torch.no_grad():
            test_output = safe_model_forward(model, test_tokenized, "åˆå§‹æµ‹è¯•")
            print(f"âœ… æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ: loss={test_output.loss}")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        raise

    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = AdamW(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    lr_scheduler = InverseSquareRootScheduler(optimizer=optimizer, warmup_steps=10000)
    
    model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)
    
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ: è®¾å¤‡={device}")
    
    # ä¸»è®­ç»ƒå¾ªç¯
    with tqdm(total=iterations, disable=not accelerator.is_main_process) as pbar:
        for iter in range(iterations):
            try:
                model.train()
                optimizer.zero_grad()
                
                # è·å–æ‰¹æ¬¡æ•°æ®
                data = next_batch(train_dataloader, device)
                tokenized_data = tokenizer(data)
                
                # ğŸ”§ CRITICAL: åœ¨æ¯æ¬¡forwardä¹‹å‰å¼ºåˆ¶ä¿®å¤æ‰€æœ‰é—®é¢˜
                with accelerator.autocast():
                    model_output = safe_model_forward(model, tokenized_data, f"è®­ç»ƒ_iter{iter}")
                    loss = model_output.loss

            # ğŸ’¡ åˆ¤æ–­æ˜¯å¦æ˜¯ NestedTensor å¹¶ä¿®å¤
                try:
                    from torch._nested import NestedTensor
                    if isinstance(loss, NestedTensor):
                        print("âš ï¸ loss æ˜¯ NestedTensorï¼Œè½¬æ¢ä¸ºæ™®é€š tensor")
                        loss = loss.to_padded_tensor(0.0).mean()
                except ImportError:
                    print("âš ï¸ å½“å‰ PyTorch ä¸æ”¯æŒ NestedTensor æ£€æµ‹ï¼Œè·³è¿‡åˆ¤æ–­")
                finally:
                    pass

                accelerator.backward(loss)
                accelerator.wait_for_everyone()
                optimizer.step()
                lr_scheduler.step()
                accelerator.wait_for_everyone()

                pbar.set_description(f'loss: {loss.item():.4f}')
                pbar.update(1)
                
                # ç®€åŒ–çš„éƒ¨åˆ†è¯„ä¼°
                if (iter+1) % partial_eval_every == 0:
                    print(f"ğŸ” ç¬¬ {iter+1} æ¬¡è¿­ä»£éƒ¨åˆ†è¯„ä¼°ï¼Œloss: {loss.item():.4f}")
                    model.eval()
                    
                    # ç®€å•è¯„ä¼°ä¸€ä¸ªbatch
                    eval_batch_count = 0
                    for batch in eval_dataloader:
                        if eval_batch_count >= 1:  # åªè¯„ä¼°1ä¸ªbatch
                            break
                        eval_batch_count += 1
                        
                        data = batch_to(batch, device)
                        tokenized_data = tokenizer(data)

                        with torch.no_grad():
                            model_output_eval = safe_model_forward(model, tokenized_data, f"éƒ¨åˆ†è¯„ä¼°_iter{iter}")
                            print(f"  è¯„ä¼°loss: {model_output_eval.loss.item():.4f}")
                
                # å®Œæ•´è¯„ä¼°ï¼ˆå¯é€‰ï¼Œé€šå¸¸ç”¨äºç”Ÿæˆä»»åŠ¡ï¼‰
                if (iter+1) % full_eval_every == 0:
                    print(f"ğŸ” ç¬¬ {iter+1} æ¬¡è¿­ä»£å®Œæ•´è¯„ä¼°...")
                    model.eval()
                    # å¯¹äºè°ƒè¯•ç‰ˆæœ¬ï¼Œè·³è¿‡å¤æ‚çš„ç”Ÿæˆè¯„ä¼°
                    print(f"  è·³è¿‡ç”Ÿæˆè¯„ä¼°ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰")
                
                # ä¿å­˜æ¨¡å‹
                if accelerator.is_main_process and (iter+1) % save_model_every == 0:
                    state = {"iter": iter, "model": model.state_dict(), "optimizer": optimizer.state_dict(), "scheduler": lr_scheduler.state_dict()}
                    if not os.path.exists(save_dir_root):
                        os.makedirs(save_dir_root)
                    torch.save(state, save_dir_root + f"checkpoint_{iter}.pt")
                    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_{iter}.pt")
                
            except Exception as e:
                print(f"\nâŒ è®­ç»ƒåœ¨ç¬¬ {iter} æ¬¡è¿­ä»£å¤±è´¥: {e}")
                traceback.print_exc()
                break
    
    print("ğŸ è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    parse_config()
    train()