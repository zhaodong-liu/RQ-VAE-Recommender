import os
import os.path as osp
import pandas as pd
import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from collections import defaultdict
from torch.utils.data import Dataset
from torch_geometric.data import download_url, extract_zip
from torch_geometric.io import fs
from typing import Callable, List, Optional, Dict, Any, Tuple
import logging
from tqdm import tqdm
from enum import Enum

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RecDataset(Enum):
    """æ¨èæ•°æ®é›†æšä¸¾"""
    ML_1M = "ml-1m"
    ML_32M = "ml-32m"

class ItemData(Dataset):
    """é€‚é…RQ-VAEçš„ç‰©å“æ•°æ®é›†ç±»"""
    
    # æ•°æ®é›†URLæ˜ å°„
    DATASET_URLS = {
        RecDataset.ML_1M: 'https://files.grouplens.org/datasets/movielens/ml-1m.zip',
        RecDataset.ML_32M: 'https://files.grouplens.org/datasets/movielens/ml-32m.zip'
    }
    
    def __init__(
        self,
        root: str,
        dataset: RecDataset = RecDataset.ML_1M,
        force_process: bool = False,
        train_test_split: str = "train",  # "train", "eval", "test", "all"
        input_dim: int = 18,
        min_interactions: int = 5,
        random_state: int = 42,
        val_ratio: float = 0.1,
        test_ratio: float = 0.1,
        normalize_features: bool = True,
        **kwargs
    ):
        self.root = root
        self.dataset = dataset
        self.force_process = force_process
        self.train_test_split = train_test_split
        self.input_dim = input_dim
        self.min_interactions = min_interactions
        self.random_state = random_state
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        self.normalize_features = normalize_features
        
        # è®¾ç½®è·¯å¾„
        self.raw_dir = osp.join(root, 'raw')
        self.processed_dir = osp.join(root, 'processed')
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.raw_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)
        
        # å¤„ç†åçš„æ–‡ä»¶è·¯å¾„
        self.processed_file = osp.join(
            self.processed_dir, 
            f'{dataset.value}_{train_test_split}_{input_dim}d.pt'
        )
        
        # åŠ è½½æˆ–å¤„ç†æ•°æ®
        if force_process or not osp.exists(self.processed_file):
            self._download_and_process()
        
        # åŠ è½½å¤„ç†åçš„æ•°æ®
        self.data = torch.load(self.processed_file)
        self.items = self.data['items']
        self.item_features = self.data['item_features']
        self.metadata = self.data['metadata']
        
        logger.info(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.items)} ä¸ªæ ·æœ¬")

    def _download_and_process(self):
        """ä¸‹è½½å¹¶å¤„ç†æ•°æ®"""
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç† {self.dataset.value} æ•°æ®é›†...")
        
        # ä¸‹è½½æ•°æ®
        if not self._check_raw_files():
            self._download()
        
        # å¤„ç†æ•°æ®
        self._process()

    def _check_raw_files(self) -> bool:
        """æ£€æŸ¥åŸå§‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        required_files = ['movies.csv', 'ratings.csv']
        if self.dataset == RecDataset.ML_1M:
            required_files = ['movies.dat', 'ratings.dat', 'users.dat']
        
        return all(osp.exists(osp.join(self.raw_dir, f)) for f in required_files)

    def _download(self):
        """ä¸‹è½½æ•°æ®é›†"""
        logger.info(f"ğŸ“¥ ä¸‹è½½ {self.dataset.value} æ•°æ®é›†...")
        
        url = self.DATASET_URLS[self.dataset]
        path = download_url(url, self.root)
        extract_zip(path, self.root)
        os.remove(path)
        
        # ç§»åŠ¨æ–‡ä»¶åˆ°rawç›®å½•
        extracted_folder = osp.join(self.root, self.dataset.value)
        if osp.exists(extracted_folder):
            # ç§»åŠ¨æ‰€æœ‰æ–‡ä»¶
            import shutil
            for file in os.listdir(extracted_folder):
                shutil.move(
                    osp.join(extracted_folder, file),
                    osp.join(self.raw_dir, file)
                )
            os.rmdir(extracted_folder)

    def _load_movielens_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """åŠ è½½MovieLensæ•°æ®"""
        if self.dataset == RecDataset.ML_1M:
            # ML-1Mä½¿ç”¨datæ–‡ä»¶å’Œ::åˆ†éš”ç¬¦
            movies = pd.read_csv(
                osp.join(self.raw_dir, 'movies.dat'),
                sep='::',
                names=['MovieID', 'Title', 'Genres'],
                engine='python',
                encoding='latin-1'
            )
            ratings = pd.read_csv(
                osp.join(self.raw_dir, 'ratings.dat'),
                sep='::',
                names=['UserID', 'MovieID', 'Rating', 'Timestamp'],
                engine='python'
            )
            users = pd.read_csv(
                osp.join(self.raw_dir, 'users.dat'),
                sep='::',
                names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'],
                engine='python'
            )
        else:
            # ML-32Mä½¿ç”¨csvæ–‡ä»¶
            movies = pd.read_csv(osp.join(self.raw_dir, 'movies.csv'))
            movies.rename(columns={'movieId': 'MovieID', 'title': 'Title', 'genres': 'Genres'}, inplace=True)
            
            ratings = pd.read_csv(osp.join(self.raw_dir, 'ratings.csv'))
            ratings.rename(columns={
                'userId': 'UserID', 
                'movieId': 'MovieID', 
                'rating': 'Rating', 
                'timestamp': 'Timestamp'
            }, inplace=True)
            
            # ML-32Mæ²¡æœ‰ç”¨æˆ·ä¿¡æ¯ï¼Œåˆ›å»ºè™šæ‹Ÿç”¨æˆ·æ•°æ®
            unique_users = ratings['UserID'].unique()
            users = pd.DataFrame({
                'UserID': unique_users,
                'Gender': 'M',  # é»˜è®¤å€¼
                'Age': 25,      # é»˜è®¤å€¼
                'Occupation': 0, # é»˜è®¤å€¼
                'Zip-code': '00000'  # é»˜è®¤å€¼
            })
        
        return movies, ratings, users

    def _create_item_features(self, movies: pd.DataFrame, ratings: pd.DataFrame) -> torch.Tensor:
        """åˆ›å»ºç‰©å“ç‰¹å¾å‘é‡"""
        logger.info("ğŸ¬ åˆ›å»ºç‰©å“ç‰¹å¾...")
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç”µå½±
        movie_ids = movies['MovieID'].unique()
        n_movies = len(movie_ids)
        
        # åˆ›å»ºç”µå½±IDåˆ°ç´¢å¼•çš„æ˜ å°„
        self.movie_id_to_idx = {mid: idx for idx, mid in enumerate(sorted(movie_ids))}
        self.idx_to_movie_id = {idx: mid for mid, idx in self.movie_id_to_idx.items()}
        
        features = []
        
        for movie_id in tqdm(sorted(movie_ids), desc="å¤„ç†ç”µå½±ç‰¹å¾"):
            movie_row = movies[movies['MovieID'] == movie_id].iloc[0]
            movie_ratings = ratings[ratings['MovieID'] == movie_id]
            
            feature_vector = []
            
            # 1. ç±»å‹ç‰¹å¾ (å¤šçƒ­ç¼–ç ï¼Œå–å‰8ä¸ªç±»å‹)
            genres = movie_row['Genres'].split('|') if pd.notna(movie_row['Genres']) else []
            all_genres = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 
                         'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 
                         'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 
                         'Thriller', 'War', 'Western']
            
            # å–å‰8ä¸ªç±»å‹ä½œä¸ºç‰¹å¾
            genre_features = [1.0 if genre in genres else 0.0 for genre in all_genres[:8]]
            feature_vector.extend(genre_features)
            
            # 2. ç»Ÿè®¡ç‰¹å¾ (10ä¸ªç‰¹å¾)
            if len(movie_ratings) > 0:
                feature_vector.extend([
                    len(movie_ratings),  # è¯„åˆ†æ•°é‡
                    movie_ratings['Rating'].mean(),  # å¹³å‡è¯„åˆ†
                    movie_ratings['Rating'].std() if len(movie_ratings) > 1 else 0.0,  # è¯„åˆ†æ ‡å‡†å·®
                    movie_ratings['Rating'].min(),  # æœ€ä½è¯„åˆ†
                    movie_ratings['Rating'].max(),  # æœ€é«˜è¯„åˆ†
                    (movie_ratings['Rating'] >= 4).sum(),  # é«˜è¯„åˆ†æ•°é‡
                    (movie_ratings['Rating'] <= 2).sum(),  # ä½è¯„åˆ†æ•°é‡
                    movie_ratings['Rating'].median(),  # ä¸­ä½æ•°è¯„åˆ†
                    movie_ratings['Rating'].quantile(0.25),  # 25%åˆ†ä½æ•°
                    movie_ratings['Rating'].quantile(0.75),  # 75%åˆ†ä½æ•°
                ])
            else:
                feature_vector.extend([0.0] * 10)
            
            features.append(feature_vector)
        
        # è½¬æ¢ä¸ºå¼ é‡
        features = torch.tensor(features, dtype=torch.float32)
        
        # å¦‚æœç‰¹å¾ç»´åº¦ä¸ç­‰äºinput_dimï¼Œè¿›è¡Œè°ƒæ•´
        if features.shape[1] != self.input_dim:
            if features.shape[1] > self.input_dim:
                # æˆªæ–­
                features = features[:, :self.input_dim]
            else:
                # å¡«å……
                padding = torch.zeros(features.shape[0], self.input_dim - features.shape[1])
                features = torch.cat([features, padding], dim=1)
        
        # æ ‡å‡†åŒ–
        if self.normalize_features:
            scaler = StandardScaler()
            features = torch.tensor(
                scaler.fit_transform(features.numpy()), 
                dtype=torch.float32
            )
        
        logger.info(f"âœ… ç‰©å“ç‰¹å¾ç»´åº¦: {features.shape}")
        return features

    def _split_data(self, ratings: pd.DataFrame) -> pd.DataFrame:
        """åˆ†å‰²æ•°æ®é›†"""
        # è¿‡æ»¤äº¤äº’æ•°ä¸è¶³çš„ç”¨æˆ·
        user_counts = ratings['UserID'].value_counts()
        valid_users = user_counts[user_counts >= self.min_interactions].index
        ratings_filtered = ratings[ratings['UserID'].isin(valid_users)]
        
        if self.train_test_split == "all":
            return ratings_filtered
        
        # ç”¨æˆ·çº§åˆ«åˆ†å‰²
        np.random.seed(self.random_state)
        unique_users = valid_users.values
        shuffled_users = np.random.permutation(unique_users)
        
        n_users = len(shuffled_users)
        n_test = int(n_users * self.test_ratio)
        n_val = int(n_users * self.val_ratio)
        
        if self.train_test_split == "train":
            target_users = shuffled_users[:-n_test-n_val]
        elif self.train_test_split == "eval":
            target_users = shuffled_users[-n_test-n_val:-n_test]
        elif self.train_test_split == "test":
            target_users = shuffled_users[-n_test:]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å‰²ç±»å‹: {self.train_test_split}")
        
        return ratings_filtered[ratings_filtered['UserID'].isin(target_users)]

    def _process(self):
        """å¤„ç†æ•°æ®çš„ä¸»å‡½æ•°"""
        # åŠ è½½åŸå§‹æ•°æ®
        movies, ratings, users = self._load_movielens_data()
        
        logger.info(f"åŸå§‹æ•°æ®: {len(movies)} ç”µå½±, {len(ratings)} è¯„åˆ†, {len(users)} ç”¨æˆ·")
        
        # åˆ›å»ºç‰©å“ç‰¹å¾
        item_features = self._create_item_features(movies, ratings)
        
        # åˆ†å‰²æ•°æ®
        split_ratings = self._split_data(ratings)
        
        # åˆ›å»ºç‰©å“äº¤äº’æ•°æ®
        items = []
        for movie_id in tqdm(sorted(movies['MovieID'].unique()), desc="åˆ›å»ºç‰©å“æ•°æ®"):
            movie_ratings = split_ratings[split_ratings['MovieID'] == movie_id]
            if len(movie_ratings) > 0:
                movie_idx = self.movie_id_to_idx[movie_id]
                
                # ç‰©å“çš„ç”¨æˆ·äº¤äº’ä¿¡æ¯
                item_data = {
                    'movie_id': movie_id,
                    'movie_idx': movie_idx,
                    'features': item_features[movie_idx],
                    'user_ratings': movie_ratings[['UserID', 'Rating']].values,
                    'n_ratings': len(movie_ratings),
                    'avg_rating': movie_ratings['Rating'].mean()
                }
                items.append(item_data)
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        processed_data = {
            'items': items,
            'item_features': item_features,
            'metadata': {
                'n_movies': len(movies),
                'n_users': len(users),
                'n_ratings': len(split_ratings),
                'input_dim': self.input_dim,
                'split': self.train_test_split,
                'movie_id_to_idx': self.movie_id_to_idx,
                'idx_to_movie_id': self.idx_to_movie_id
            }
        }
        
        torch.save(processed_data, self.processed_file)
        logger.info(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼Œä¿å­˜è‡³: {self.processed_file}")

    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.items)

    def __getitem__(self, idx: int) -> torch.Tensor:
        """è·å–å•ä¸ªæ ·æœ¬ - è¿”å›ç‰©å“ç‰¹å¾å‘é‡"""
        if isinstance(idx, torch.Tensor):
            # å¦‚æœidxæ˜¯å¼ é‡ï¼Œå¤„ç†æ‰¹é‡ç´¢å¼•
            return self.item_features[idx]
        else:
            # å•ä¸ªç´¢å¼•
            return self.item_features[idx]

    def get_item_by_movie_id(self, movie_id: int) -> Optional[torch.Tensor]:
        """æ ¹æ®ç”µå½±IDè·å–ç‰¹å¾"""
        if movie_id in self.movie_id_to_idx:
            idx = self.movie_id_to_idx[movie_id]
            return self.item_features[idx]
        return None

    def get_all_features(self) -> torch.Tensor:
        """è·å–æ‰€æœ‰ç‰©å“ç‰¹å¾"""
        return self.item_features

    def get_random_batch(self, batch_size: int) -> torch.Tensor:
        """è·å–éšæœºæ‰¹æ¬¡"""
        indices = torch.randint(0, len(self), (batch_size,))
        return self.item_features[indices]


# æ•°æ®å·¥å…·å‡½æ•° - ä¸è®­ç»ƒä»£ç å…¼å®¹
def batch_to(batch, device):
    """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
    if isinstance(batch, torch.Tensor):
        return batch.to(device)
    elif isinstance(batch, (list, tuple)):
        return [batch_to(item, device) for item in batch]
    elif isinstance(batch, dict):
        return {key: batch_to(value, device) for key, value in batch.items()}
    else:
        return batch

def cycle(dataloader):
    """æ— é™å¾ªç¯æ•°æ®åŠ è½½å™¨"""
    while True:
        for batch in dataloader:
            yield batch

def next_batch(dataloader, device):
    """è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡"""
    batch = next(dataloader)
    return batch_to(batch, device)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®é›†
    dataset = ItemData(
        root="dataset/ml-1m",
        dataset=RecDataset.ML_1M,
        train_test_split="train",
        input_dim=18,
        force_process=True
    )
    
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"ç‰¹å¾ç»´åº¦: {dataset.item_features.shape}")
    print(f"æ ·æœ¬ç‰¹å¾: {dataset[0]}")