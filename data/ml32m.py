import os
import os.path as osp
import pandas as pd
import torch
import numpy as np

from data.preprocessing import PreprocessingMixin
from torch_geometric.data import HeteroData
from torch_geometric.data import InMemoryDataset
from torch_geometric.data import download_url
from torch_geometric.data import extract_zip
from torch_geometric.io import fs
from typing import Callable, List, Optional
import logging
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MovieLens32M(InMemoryDataset):
    url = 'https://files.grouplens.org/datasets/movielens/ml-32m.zip'

    def __init__(
        self,
        root: str,
        transform: Optional[Callable] = None,
        pre_transform: Optional[Callable] = None,
        force_reload: bool = False,
    ) -> None:
        super().__init__(root, transform, pre_transform, force_reload=force_reload)
        # ä¿®å¤bug: ä½¿ç”¨å…¼å®¹çš„åŠ è½½æ–¹å¼
        try:
            self.load(self.processed_paths[0], data_cls=HeteroData)
        except Exception as e:
            logger.warning(f"æ ‡å‡†åŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•å…¼å®¹æ¨¡å¼")
            # å…¼å®¹æ¨¡å¼åŠ è½½
            self._load_compatible()
    
    def _load_compatible(self):
        """å…¼å®¹æ¨¡å¼åŠ è½½ï¼Œé¿å…weights_onlyé—®é¢˜"""
        if osp.exists(self.processed_paths[0]):
            # ç›´æ¥ä½¿ç”¨torch.loadï¼Œä¸ä½¿ç”¨weights_only
            self.data, self.slices = torch.load(
                self.processed_paths[0], 
                map_location='cpu',
                weights_only=False  # ä¿®å¤: ç¦ç”¨weights_only
            )
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¤„ç†åçš„æ•°æ®æ–‡ä»¶: {self.processed_paths[0]}")
    
    @property
    def raw_file_names(self) -> List[str]:
        return ['links.csv', 'movies.csv', 'ratings.csv', 'tags.csv']
    
    @property
    def processed_file_names(self) -> str:
        return 'data.pt'

    @property
    def has_process(self) -> bool:
        return not os.path.exists(self.processed_paths[0])
    
    def download(self) -> None:
        path = download_url(self.url, self.root)
        extract_zip(path, self.root)
        os.remove(path)
        folder = osp.join(self.root, 'ml-32m')
        fs.rm(self.raw_dir)
        os.rename(folder, self.raw_dir)

    def process(self):
        """åŸºç±»processæ–¹æ³• - ä¿®å¤bug: ä¸èƒ½ä¸ºç©º"""
        raise NotImplementedError("Subclasses must implement process method")


class RawMovieLens32M(MovieLens32M, PreprocessingMixin):
    """ä¿®å¤åçš„åŸå§‹MovieLens32MåŠ è½½å™¨ï¼Œ768ç»´è¾“å‡º"""
    
    def __init__(
        self,
        root,
        transform=None,
        pre_transform=None,
        force_reload=False,
        split=None,
        # å‚æ•°é…ç½®
        target_dim=768,  # ä¿®å¤: è®¾ç½®ç›®æ ‡ç»´åº¦ä¸º768
        max_seq_len=200,
        min_rating_count=5,
        train_split=0.8,
        text_model_name='sentence-transformers/sentence-t5-xl'
    ) -> None:
        self.split = split
        self.target_dim = target_dim
        self.max_seq_len = max_seq_len
        self.min_rating_count = min_rating_count
        self.train_split = train_split
        self.text_model_name = text_model_name
        
        super(RawMovieLens32M, self).__init__(
            root, transform, pre_transform, force_reload
        )

    def _load_ratings(self):
        """åŠ è½½è¯„åˆ†æ•°æ®"""
        return pd.read_csv(self.raw_paths[2])
    
    def _load_movies(self):
        """åŠ è½½ç”µå½±æ•°æ®"""
        return pd.read_csv(self.raw_paths[1])
    
    def _create_simple_titles(self, movies_df: pd.DataFrame) -> List[str]:
        """æå–ç®€æ´çš„ç”µå½±æ ‡é¢˜ (å›å½’åŸå§‹ç‰ˆæœ¬é£æ ¼)"""
        logger.info("ğŸ¬ æå–ç”µå½±æ ‡é¢˜...")
        
        # ç®€å•æå–æ ‡é¢˜ï¼Œä¿æŒåŸå§‹ç‰ˆæœ¬çš„ç®€æ´æ€§
        titles = movies_df["title"].apply(
            lambda s: s.split("(")[0].strip() if pd.notna(s) and '(' in s else (s if pd.notna(s) else "Unknown")
        ).tolist()
        
        return titles

    def _adjust_feature_dimension(self, features: torch.Tensor) -> torch.Tensor:
        """è°ƒæ•´ç‰¹å¾ç»´åº¦åˆ°ç›®æ ‡ç»´åº¦"""
        current_dim = features.shape[1]
        
        if current_dim == self.target_dim:
            return features
        elif current_dim > self.target_dim:
            # æˆªæ–­åˆ°ç›®æ ‡ç»´åº¦
            logger.info(f"ğŸ”§ æˆªæ–­ç‰¹å¾ç»´åº¦: {current_dim} -> {self.target_dim}")
            return features[:, :self.target_dim]
        else:
            # å¡«å……åˆ°ç›®æ ‡ç»´åº¦
            logger.info(f"ğŸ”§ å¡«å……ç‰¹å¾ç»´åº¦: {current_dim} -> {self.target_dim}")
            padding_dim = self.target_dim - current_dim
            padding = torch.zeros(features.shape[0], padding_dim, dtype=features.dtype)
            return torch.cat([features, padding], dim=1)

    def _encode_text_safe(self, text_list: List[str]) -> torch.Tensor:
        """å®‰å…¨çš„æ–‡æœ¬ç¼–ç ï¼Œå¸¦é”™è¯¯å¤„ç†"""
        try:
            logger.info(f"ğŸ”¤ ä½¿ç”¨ {self.text_model_name} ç¼–ç æ–‡æœ¬ç‰¹å¾...")
            embeddings = self._encode_text_feature(text_list)
            return embeddings
        except Exception as e:
            logger.warning(f"âš ï¸ æ–‡æœ¬ç¼–ç å¤±è´¥: {e}ï¼Œä½¿ç”¨éšæœºåµŒå…¥")
            # åˆ›å»ºéšæœºåµŒå…¥ä½œä¸ºå¤‡é€‰
            torch.manual_seed(42)
            n_items = len(text_list)
            embeddings = torch.randn(n_items, 384)  # MiniLMé»˜è®¤384ç»´
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
            return embeddings

    def process(self, max_seq_len=None) -> None:
        """ä¸»å¤„ç†å‡½æ•°ï¼Œè¾“å‡º768ç»´ç‰¹å¾"""
        if max_seq_len is not None:
            self.max_seq_len = max_seq_len
            
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†MovieLensæ•°æ®")
        logger.info(f"ç›®æ ‡ç‰¹å¾ç»´åº¦: {self.target_dim}")
        logger.info(f"åºåˆ—é•¿åº¦: {self.max_seq_len}")
        
        data = HeteroData()
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        ratings_df = self._load_ratings()
        movies_df = self._load_movies()
        
        logger.info(f"åŸå§‹æ•°æ®: {len(movies_df)} ç”µå½±, {len(ratings_df)} è¯„åˆ†")

        # 2. å¤„ç†ç”µå½±æ•°æ®
        logger.info("ğŸ­ å¤„ç†ç”µå½±ç‰¹å¾...")
        
        # è®¾ç½®movieIdä¸ºç´¢å¼•
        movies_indexed = movies_df.set_index('movieId')
        movie_mapping = {idx: i for i, idx in enumerate(movies_indexed.index)}
        
        # å¤„ç†ç±»å‹ç‰¹å¾
        try:
            genres_dummies = movies_indexed["genres"].str.get_dummies('|').values
            genres = self._process_genres(genres_dummies, one_hot=True)
            genres = torch.from_numpy(genres).to(torch.float)
        except Exception as e:
            logger.warning(f"ç±»å‹å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨é›¶å‘é‡")
            genres = torch.zeros(len(movies_indexed), 20)  # é»˜è®¤20ä¸ªç±»å‹

        # åˆ›å»ºç®€æ´æ ‡é¢˜ (å›å½’åŸå§‹é£æ ¼)
        titles = self._create_simple_titles(movies_df)
        
        # å®‰å…¨çš„æ–‡æœ¬ç¼–ç  (åªç¼–ç æ ‡é¢˜ï¼Œä¸æ˜¯å¤æ‚æè¿°)
        titles_emb = self._encode_text_safe(titles)

        # åˆå¹¶ç‰¹å¾
        logger.info(f"æ–‡æœ¬åµŒå…¥ç»´åº¦: {titles_emb.shape}")
        logger.info(f"ç±»å‹ç‰¹å¾ç»´åº¦: {genres.shape}")
        
        # ç¡®ä¿ç»´åº¦å…¼å®¹
        min_rows = min(titles_emb.shape[0], genres.shape[0])
        titles_emb = titles_emb[:min_rows]
        genres = genres[:min_rows]
        
        x = torch.cat([titles_emb, genres], dim=1)
        
        # è°ƒæ•´åˆ°ç›®æ ‡ç»´åº¦
        x = self._adjust_feature_dimension(x)
        
        # è®¾ç½®itemèŠ‚ç‚¹ç‰¹å¾
        data['item'].x = x
        data['item'].text = np.array(titles[:min_rows])  # ä½¿ç”¨ç®€æ´æ ‡é¢˜
        
        logger.info(f"âœ… æœ€ç»ˆç”µå½±ç‰¹å¾ç»´åº¦: {x.shape}")

        # 3. å¤„ç†ç”¨æˆ·æ•°æ®
        logger.info("ğŸ‘¥ å¤„ç†ç”¨æˆ·æ•°æ®...")
        
        # ç§»é™¤ä½é¢‘ç”¨æˆ·
        try:
            full_user_df = pd.DataFrame({"userId": ratings_df["userId"].unique()})
            filtered_user_df = self._remove_low_occurrence(
                ratings_df, full_user_df, "userId"
            )
            user_mapping = {idx: i for i, idx in enumerate(filtered_user_df["userId"])}
        except Exception as e:
            logger.warning(f"ç”¨æˆ·è¿‡æ»¤å¤±è´¥: {e}ï¼Œä½¿ç”¨æ‰€æœ‰ç”¨æˆ·")
            unique_users = ratings_df["userId"].unique()
            user_mapping = {idx: i for i, idx in enumerate(unique_users)}
        
        data['user'].num_nodes = len(user_mapping)
        logger.info(f"âœ… ç”¨æˆ·æ•°é‡: {len(user_mapping)}")

        # 4. å¤„ç†è¯„åˆ†æ•°æ®
        logger.info("â­ å¤„ç†è¯„åˆ†æ•°æ®...")
        
        # å®‰å…¨çš„è¯„åˆ†è¿‡æ»¤
        try:
            filtered_ratings_df = self._remove_low_occurrence(
                ratings_df, ratings_df, ["userId", "movieId"]
            )
        except Exception as e:
            logger.warning(f"è¯„åˆ†è¿‡æ»¤å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
            filtered_ratings_df = ratings_df
        
        # æ„å»ºè¾¹ç´¢å¼•ï¼ˆæ·»åŠ å®‰å…¨æ£€æŸ¥ï¼‰
        valid_pairs = []
        for _, row in filtered_ratings_df.iterrows():
            user_id = row['userId']
            movie_id = row['movieId']
            if user_id in user_mapping and movie_id in movie_mapping:
                valid_pairs.append((user_mapping[user_id], movie_mapping[movie_id], row))
        
        if not valid_pairs:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ç”¨æˆ·-ç”µå½±å¯¹!")
            return
        
        src, dst, rating_data = zip(*valid_pairs)
        
        edge_index = torch.tensor([list(src), list(dst)], dtype=torch.long)
        rating = torch.tensor([row['rating'] for row in rating_data], dtype=torch.long)
        time = torch.tensor([row['timestamp'] for row in rating_data], dtype=torch.long)
        
        data['user', 'rates', 'item'].edge_index = edge_index
        data['user', 'rates', 'item'].rating = rating
        data['user', 'rates', 'item'].time = time

        # åå‘è¾¹
        data['item', 'rated_by', 'user'].edge_index = edge_index.flip([0])
        data['item', 'rated_by', 'user'].rating = rating
        data['item', 'rated_by', 'user'].time = time
        
        logger.info(f"âœ… äº¤äº’è¾¹æ•°é‡: {edge_index.shape[1]}")

        # 5. ç”Ÿæˆç”¨æˆ·å†å²åºåˆ— (ä½¿ç”¨PreprocessingMixinçš„å¼ºå¤§åŠŸèƒ½)
        logger.info("ğŸ“š ç”Ÿæˆç”¨æˆ·å†å²åºåˆ—...")
        
        try:
            # å‡†å¤‡åºåˆ—æ•°æ®
            sequence_df = pd.DataFrame({
                'userId': [row['userId'] for row in rating_data],
                'movieId': [row['movieId'] for row in rating_data],
                'rating': [row['rating'] for row in rating_data],
                'timestamp': [row['timestamp'] for row in rating_data]
            })
            
            sequence_df["itemId"] = sequence_df["movieId"].apply(lambda x: movie_mapping.get(x, -1))
            sequence_df = sequence_df[sequence_df["itemId"] != -1]
            
            # è¯„åˆ†å¤„ç† (ä¿æŒåŸå§‹ç‰ˆæœ¬çš„ç¼©æ”¾)
            sequence_df["rating"] = (2 * sequence_df["rating"]).astype(int)  # åŸç‰ˆçš„è¯„åˆ†ç¼©æ”¾
            
            # ä½¿ç”¨PreprocessingMixinçš„å¼ºå¤§æ–¹æ³• (ä¿æŒåŸç‰ˆå‚æ•°)
            history = self._generate_user_history(
                sequence_df,
                features=["itemId", "rating"],
                window_size=self.max_seq_len,
                stride=max(1, self.max_seq_len // 4),  # åŸç‰ˆä½¿ç”¨180ï¼Œè¿™é‡ŒåŠ¨æ€è®¡ç®—
                train_split=self.train_split
            )
            data["user", "rated", "item"].history = history
            logger.info("âœ… ç”¨æˆ·å†å²åºåˆ—ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åºåˆ—ç”Ÿæˆå¤±è´¥: {e}")
            # å¦‚æœPreprocessingMixinæ–¹æ³•å¤±è´¥ï¼Œè¯´æ˜æ•°æ®æœ‰ä¸¥é‡é—®é¢˜
            # è¿™æ—¶åº”è¯¥æ£€æŸ¥æ•°æ®è€Œä¸æ˜¯ç”¨ç®€åŒ–ç‰ˆæœ¬æ©ç›–é—®é¢˜
            raise RuntimeError(f"åºåˆ—ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®: {e}")

        # 6. æ·»åŠ è®­ç»ƒæ©ç 
        gen = torch.Generator()
        gen.manual_seed(42)
        data['item'].is_train = torch.rand(x.shape[0], generator=gen) > 0.05
        
        # 7. æ•°æ®ç±»å‹ç¡®ä¿
        self._ensure_data_types(data)
        
        # 8. åº”ç”¨é¢„å˜æ¢
        if self.pre_transform is not None:
            data = self.pre_transform(data)

        # 9. ä¿å­˜æ•°æ®ï¼ˆä½¿ç”¨å…¼å®¹æ¨¡å¼ï¼‰
        logger.info("ğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
        torch.save([data], self.processed_paths[0])  # ç®€åŒ–ä¿å­˜
        
        logger.info("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆ!")
        self._print_data_summary(data)

    def _ensure_data_types(self, data):
        """ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®"""
        # ç¡®ä¿ç‰¹å¾æ˜¯floatç±»å‹
        if hasattr(data['item'], 'x'):
            data['item'].x = data['item'].x.float()
        
        # ç¡®ä¿è¾¹ç´¢å¼•æ˜¯longç±»å‹
        if hasattr(data['user', 'rates', 'item'], 'edge_index'):
            data['user', 'rates', 'item'].edge_index = data['user', 'rates', 'item'].edge_index.long()
        
        # ç¡®ä¿è¯„åˆ†æ˜¯longç±»å‹
        if hasattr(data['user', 'rates', 'item'], 'rating'):
            data['user', 'rates', 'item'].rating = data['user', 'rates', 'item'].rating.long()

    def _print_data_summary(self, data):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        logger.info("ğŸ“Š æ•°æ®æ‘˜è¦:")
        logger.info(f"- ç”µå½±èŠ‚ç‚¹: {data['item'].x.shape[0]}")
        logger.info(f"- ç”µå½±ç‰¹å¾ç»´åº¦: {data['item'].x.shape[1]}")
        logger.info(f"- ç”¨æˆ·èŠ‚ç‚¹: {data['user'].num_nodes}")
        logger.info(f"- äº¤äº’è¾¹: {data['user', 'rates', 'item'].edge_index.shape[1]}")
        
        if 'history' in data["user", "rated", "item"]:
            history = data["user", "rated", "item"].history
            if 'train' in history:
                logger.info(f"- è®­ç»ƒåºåˆ—: {len(history['train'].get('itemId', []))}")
            if 'eval' in history:
                logger.info(f"- è¯„ä¼°åºåˆ—: {len(history['eval'].get('itemId', []))}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 768ç»´ç‰ˆæœ¬
    dataset = RawMovieLens32M(
        root="dataset/ml-32m",
        force_reload=True,
        target_dim=768,  # 768ç»´è¾“å‡º
        max_seq_len=200,
        min_rating_count=5,
        train_split=0.8,
        text_model_name='sentence-transformers/sentence-t5-xl'

    
    print("âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
    data = dataset[0]
    print(f"ç”µå½±ç‰¹å¾å½¢çŠ¶: {data['item'].x.shape}")  # åº”è¯¥æ˜¯ [N, 768]
    print(f"ç”¨æˆ·æ•°é‡: {data['user'].num_nodes}")
    print(f"äº¤äº’è¾¹æ•°é‡: {data['user', 'rates', 'item'].edge_index.shape[1]}")