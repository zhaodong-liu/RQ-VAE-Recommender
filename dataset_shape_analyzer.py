import torch
import numpy as np
from torch_geometric.data import HeteroData
import os
from typing import Dict, Any, List, Tuple
import pandas as pd

class DatasetShapeAnalyzer:
    """æ•°æ®é›†ç»“æ„å’Œshapeåˆ†æå™¨"""
    
    def __init__(self):
        self.analysis_results = {}
    
    def analyze_tensor_shape(self, data: Any, name: str = "", max_depth: int = 3, current_depth: int = 0) -> Dict:
        """é€’å½’åˆ†ææ•°æ®ç»“æ„å’Œshape"""
        result = {
            'name': name,
            'type': str(type(data).__name__),
            'shape': None,
            'dtype': None,
            'size': None,
            'children': {}
        }
        
        if current_depth >= max_depth:
            result['note'] = 'Max depth reached'
            return result
        
        try:
            # Tensorç±»å‹
            if torch.is_tensor(data):
                result['shape'] = list(data.shape)
                result['dtype'] = str(data.dtype)
                result['size'] = data.numel()
                result['device'] = str(data.device)
                
                # å¦‚æœæ˜¯å°å¼ é‡ï¼Œæ˜¾ç¤ºéƒ¨åˆ†æ•°æ®
                if data.numel() <= 20:
                    result['sample_data'] = data.tolist()
                else:
                    result['sample_data'] = f"First 5 elements: {data.flatten()[:5].tolist()}"
            
            # NumPyæ•°ç»„
            elif isinstance(data, np.ndarray):
                result['shape'] = list(data.shape)
                result['dtype'] = str(data.dtype)
                result['size'] = data.size
                if data.size <= 20:
                    result['sample_data'] = data.tolist()
                else:
                    result['sample_data'] = f"First 5 elements: {data.flatten()[:5].tolist()}"
            
            # å­—å…¸ç±»å‹
            elif isinstance(data, dict):
                result['length'] = len(data)
                result['keys'] = list(data.keys())
                for key, value in data.items():
                    if current_depth < max_depth - 1:
                        result['children'][str(key)] = self.analyze_tensor_shape(
                            value, str(key), max_depth, current_depth + 1
                        )
            
            # åˆ—è¡¨æˆ–å…ƒç»„
            elif isinstance(data, (list, tuple)):
                result['length'] = len(data)
                if len(data) > 0:
                    # åˆ†æå‰å‡ ä¸ªå…ƒç´ çš„ç±»å‹
                    first_elem = data[0]
                    result['element_type'] = str(type(first_elem).__name__)
                    
                    if len(data) <= 10:  # å¦‚æœåˆ—è¡¨è¾ƒçŸ­ï¼Œåˆ†ææ‰€æœ‰å…ƒç´ 
                        for i, item in enumerate(data[:5]):  # åªåˆ†æå‰5ä¸ª
                            result['children'][f'[{i}]'] = self.analyze_tensor_shape(
                                item, f'[{i}]', max_depth, current_depth + 1
                            )
                    else:
                        # åˆ†æç¬¬ä¸€ä¸ªå…ƒç´ 
                        result['children']['[0]'] = self.analyze_tensor_shape(
                            first_elem, '[0]', max_depth, current_depth + 1
                        )
            
            # HeteroDataç‰¹æ®Šå¤„ç†
            elif isinstance(data, HeteroData):
                result['node_types'] = list(data.node_types)
                result['edge_types'] = [str(edge_type) for edge_type in data.edge_types]
                result['metadata'] = str(data.metadata())
                
                # åˆ†æèŠ‚ç‚¹æ•°æ®
                for node_type in data.node_types:
                    node_data = data[node_type]
                    result['children'][f'node_{node_type}'] = self.analyze_tensor_shape(
                        node_data, f'node_{node_type}', max_depth, current_depth + 1
                    )
                
                # åˆ†æè¾¹æ•°æ®
                for edge_type in data.edge_types:
                    edge_data = data[edge_type]
                    edge_key = f"edge_{edge_type[0]}__{edge_type[1]}__{edge_type[2]}"
                    result['children'][edge_key] = self.analyze_tensor_shape(
                        edge_data, edge_key, max_depth, current_depth + 1
                    )
            
            # å…¶ä»–å¯¹è±¡
            else:
                if hasattr(data, '__len__'):
                    result['length'] = len(data)
                
                # å°è¯•è·å–å¸¸è§å±æ€§
                common_attrs = ['x', 'y', 'edge_index', 'edge_attr', 'batch', 'ptr']
                for attr in common_attrs:
                    if hasattr(data, attr):
                        attr_data = getattr(data, attr)
                        if attr_data is not None:
                            result['children'][attr] = self.analyze_tensor_shape(
                                attr_data, attr, max_depth, current_depth + 1
                            )
        
        except Exception as e:
            result['error'] = str(e)
        
        return result
    
    def print_analysis(self, analysis: Dict, indent: str = "") -> None:
        """æ‰“å°åˆ†æç»“æœ"""
        name = analysis['name']
        type_name = analysis['type']
        
        print(f"{indent}ğŸ“Š {name} ({type_name})")
        
        if analysis.get('shape'):
            print(f"{indent}  Shape: {analysis['shape']}")
        if analysis.get('dtype'):
            print(f"{indent}  Dtype: {analysis['dtype']}")
        if analysis.get('size'):
            print(f"{indent}  Size: {analysis['size']:,}")
        if analysis.get('device'):
            print(f"{indent}  Device: {analysis['device']}")
        if analysis.get('length'):
            print(f"{indent}  Length: {analysis['length']}")
        if analysis.get('keys'):
            print(f"{indent}  Keys: {analysis['keys']}")
        if analysis.get('node_types'):
            print(f"{indent}  Node Types: {analysis['node_types']}")
        if analysis.get('edge_types'):
            print(f"{indent}  Edge Types: {analysis['edge_types']}")
        if analysis.get('sample_data'):
            print(f"{indent}  Sample: {analysis['sample_data']}")
        if analysis.get('error'):
            print(f"{indent}  âŒ Error: {analysis['error']}")
        
        # é€’å½’æ‰“å°å­ç»“æ„
        if analysis.get('children'):
            for child_name, child_analysis in analysis['children'].items():
                print()
                self.print_analysis(child_analysis, indent + "  ")
    
    def compare_datasets(self, data1: Any, data2: Any, name1: str, name2: str) -> None:
        """æ¯”è¾ƒä¸¤ä¸ªæ•°æ®é›†çš„ç»“æ„"""
        print("="*80)
        print(f"ğŸ” æ•°æ®é›†ç»“æ„å¯¹æ¯”: {name1} vs {name2}")
        print("="*80)
        
        analysis1 = self.analyze_tensor_shape(data1, name1)
        analysis2 = self.analyze_tensor_shape(data2, name2)
        
        print(f"\nğŸ“ˆ {name1} æ•°æ®ç»“æ„:")
        print("-" * 50)
        self.print_analysis(analysis1)
        
        print(f"\nğŸ“ˆ {name2} æ•°æ®ç»“æ„:")
        print("-" * 50)
        self.print_analysis(analysis2)
        
        # ç»“æ„å·®å¼‚åˆ†æ
        print(f"\nğŸ”„ ç»“æ„å·®å¼‚åˆ†æ:")
        print("-" * 50)
        self._compare_structures(analysis1, analysis2, name1, name2)
    
    def _compare_structures(self, analysis1: Dict, analysis2: Dict, name1: str, name2: str) -> None:
        """æ¯”è¾ƒä¸¤ä¸ªåˆ†æç»“æœçš„å·®å¼‚"""
        print(f"ç±»å‹: {name1}({analysis1['type']}) vs {name2}({analysis2['type']})")
        
        if analysis1.get('shape') and analysis2.get('shape'):
            print(f"Shape: {name1}{analysis1['shape']} vs {name2}{analysis2['shape']}")
        
        if analysis1.get('keys') and analysis2.get('keys'):
            keys1 = set(analysis1['keys'])
            keys2 = set(analysis2['keys'])
            common = keys1 & keys2
            only1 = keys1 - keys2
            only2 = keys2 - keys1
            
            print(f"å…±åŒkeys ({len(common)}): {sorted(common)}")
            if only1:
                print(f"ä»…{name1}æœ‰ ({len(only1)}): {sorted(only1)}")
            if only2:
                print(f"ä»…{name2}æœ‰ ({len(only2)}): {sorted(only2)}")

def safe_torch_load(data_path: str) -> Any:
    """å®‰å…¨åŠ è½½torchæ•°æ®æ–‡ä»¶ï¼Œå…¼å®¹PyTorch 2.6+"""
    try:
        # å°è¯•ä½¿ç”¨æ–°çš„å®‰å…¨åŠ è½½æ–¹å¼
        from numpy.core.multiarray import _reconstruct
        import torch.serialization
        
        # æ–¹æ³•1: ä½¿ç”¨å®‰å…¨çš„å…¨å±€å˜é‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        with torch.serialization.safe_globals([_reconstruct]):
            raw_data = torch.load(data_path, weights_only=True)
        data = extract_actual_data(raw_data)
        return data
    except:
        try:
            # æ–¹æ³•2: å¦‚æœä¸Šé¢å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼ˆéœ€è¦ä¿¡ä»»æ•°æ®æºï¼‰
            print("âš ï¸  ä½¿ç”¨ä¼ ç»ŸåŠ è½½æ–¹å¼ (weights_only=False)")
            raw_data = torch.load(data_path, weights_only=False)
            data = extract_actual_data(raw_data)
            return data
        except Exception as e:
            print(f"âŒ æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥: {e}")
            return None

def extract_actual_data(raw_data: Any) -> Any:
    """ä»PyTorch Geometricä¿å­˜çš„æ•°æ®ä¸­æå–å®é™…çš„æ•°æ®å¯¹è±¡"""
    # PyTorch Geometricçš„InMemoryDatasetä¿å­˜æ ¼å¼é€šå¸¸æ˜¯tuple: (data_list, slices, data_cls)
    if isinstance(raw_data, tuple):
        print(f"ğŸ” æ£€æµ‹åˆ°tupleæ ¼å¼æ•°æ®ï¼Œé•¿åº¦: {len(raw_data)}")
        
        # é€šå¸¸ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯æ•°æ®åˆ—è¡¨æˆ–å®é™…æ•°æ®
        if len(raw_data) > 0:
            first_elem = raw_data[0]
            
            # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åˆ—è¡¨ä¸”åŒ…å«æ•°æ®
            if isinstance(first_elem, list) and len(first_elem) > 0:
                print(f"âœ… æå–åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæ•°æ®å¯¹è±¡")
                return first_elem[0]
            
            # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å­—å…¸ï¼ˆå¯èƒ½åŒ…å«HeteroDataçš„å­˜å‚¨ï¼‰
            elif isinstance(first_elem, dict):
                print(f"âœ… æ£€æµ‹åˆ°å­—å…¸æ ¼å¼ï¼Œå°è¯•é‡æ„HeteroData")
                return reconstruct_hetero_data_from_dict(first_elem)
            
            # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ ç›´æ¥æ˜¯æ•°æ®å¯¹è±¡
            elif first_elem is not None and not isinstance(first_elem, type):
                print(f"âœ… æå–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºæ•°æ®å¯¹è±¡")
                return first_elem
            
            # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯Noneï¼Œå°è¯•å…¶ä»–å…ƒç´ 
            else:
                for i, elem in enumerate(raw_data):
                    if elem is not None and not isinstance(elem, type):
                        if isinstance(elem, dict):
                            print(f"âœ… ä»ç¬¬{i}ä¸ªå…ƒç´ é‡æ„HeteroData")
                            return reconstruct_hetero_data_from_dict(elem)
                        elif hasattr(elem, '__dict__'):
                            print(f"âœ… æå–ç¬¬{i}ä¸ªå…ƒç´ ä½œä¸ºæ•°æ®å¯¹è±¡")
                            return elem
    
    # å¦‚æœä¸æ˜¯tupleæˆ–æ— æ³•æå–ï¼Œè¿”å›åŸå§‹æ•°æ®
    print(f"ğŸ”„ è¿”å›åŸå§‹æ•°æ®æ ¼å¼: {type(raw_data)}")
    return raw_data

def reconstruct_hetero_data_from_dict(data_dict: dict) -> HeteroData:
    """ä»å­—å…¸é‡æ„HeteroDataå¯¹è±¡"""
    try:
        hetero_data = HeteroData()
        
        # å¤„ç†èŠ‚ç‚¹æ•°æ®
        for key, value in data_dict.items():
            if key == '_global_store':
                continue
            elif isinstance(key, str) and key in ['item', 'user']:
                # èŠ‚ç‚¹æ•°æ®
                node_type = key
                if isinstance(value, dict):
                    for attr_name, attr_value in value.items():
                        setattr(hetero_data[node_type], attr_name, attr_value)
                        print(f"  æ·»åŠ èŠ‚ç‚¹ {node_type}.{attr_name}: {type(attr_value)}")
            elif isinstance(key, tuple) and len(key) == 3:
                # è¾¹æ•°æ®
                edge_type = key
                if isinstance(value, dict):
                    for attr_name, attr_value in value.items():
                        setattr(hetero_data[edge_type], attr_name, attr_value)
                        print(f"  æ·»åŠ è¾¹ {edge_type}.{attr_name}: {type(attr_value)}")
        
        print(f"âœ… æˆåŠŸé‡æ„HeteroData")
        return hetero_data
        
    except Exception as e:
        print(f"âŒ HeteroDataé‡æ„å¤±è´¥: {e}")
        return data_dict

def analyze_ml32m_data(data_path: str) -> Any:
    """åŠ è½½å’Œåˆ†æML32Mæ•°æ®"""
    print("ğŸ¬ åŠ è½½MovieLens32Mæ•°æ®...")
    try:
        if os.path.exists(data_path):
            data = safe_torch_load(data_path)
            if data is not None:
                print(f"âœ… æˆåŠŸåŠ è½½ML32Mæ•°æ®ä»: {data_path}")
                return data
            else:
                print(f"âŒ åŠ è½½å¤±è´¥: {data_path}")
                return None
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return None
    except Exception as e:
        print(f"âŒ åŠ è½½ML32Mæ•°æ®å¤±è´¥: {e}")
        return None

def analyze_amazon_data(data_path: str) -> Any:
    """åŠ è½½å’Œåˆ†æAmazonæ•°æ®"""
    print("ğŸ›’ åŠ è½½Amazonæ•°æ®...")
    try:
        if os.path.exists(data_path):
            data = safe_torch_load(data_path)
            if data is not None:
                print(f"âœ… æˆåŠŸåŠ è½½Amazonæ•°æ®ä»: {data_path}")
                return data
            else:
                print(f"âŒ åŠ è½½å¤±è´¥: {data_path}")
                return None
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return None
    except Exception as e:
        print(f"âŒ åŠ è½½Amazonæ•°æ®å¤±è´¥: {e}")
        return None

def detailed_hetero_analysis(data: HeteroData, dataset_name: str) -> None:
    """è¯¦ç»†åˆ†æHeteroDataç»“æ„"""
    print(f"\nğŸ”¬ {dataset_name} HeteroDataè¯¦ç»†åˆ†æ:")
    print("=" * 60)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"  èŠ‚ç‚¹ç±»å‹: {data.node_types if hasattr(data, 'node_types') else 'æœªçŸ¥'}")
    print(f"  è¾¹ç±»å‹: {[str(et) for et in data.edge_types] if hasattr(data, 'edge_types') else 'æœªçŸ¥'}")
    
    # èŠ‚ç‚¹åˆ†æ
    print("\nğŸ“¦ èŠ‚ç‚¹ç±»å‹åˆ†æ:")
    if hasattr(data, 'node_types'):
        for node_type in data.node_types:
            node_data = data[node_type]
            print(f"  {node_type}:")
            for attr_name in dir(node_data):
                if not attr_name.startswith('_') and attr_name not in ['keys', 'items', 'values']:
                    try:
                        attr_value = getattr(node_data, attr_name)
                        if torch.is_tensor(attr_value):
                            print(f"    {attr_name}: {list(attr_value.shape)} ({attr_value.dtype})")
                        elif isinstance(attr_value, np.ndarray):
                            print(f"    {attr_name}: {list(attr_value.shape)} ({attr_value.dtype})")
                        elif attr_value is not None and not callable(attr_value):
                            if hasattr(attr_value, '__len__'):
                                print(f"    {attr_name}: {type(attr_value)} (len={len(attr_value)})")
                            else:
                                print(f"    {attr_name}: {type(attr_value)}")
                    except:
                        continue
    else:
        # å¦‚æœæ²¡æœ‰æ ‡å‡†çš„node_typesï¼Œå°è¯•æ£€æŸ¥å¸¸è§çš„èŠ‚ç‚¹ç±»å‹
        common_nodes = ['user', 'item']
        for node_type in common_nodes:
            if hasattr(data, '__getitem__'):
                try:
                    node_data = data[node_type]
                    print(f"  {node_type}: æ‰¾åˆ°èŠ‚ç‚¹æ•°æ®")
                    if hasattr(node_data, '__dict__'):
                        for attr_name, attr_value in node_data.__dict__.items():
                            if torch.is_tensor(attr_value):
                                print(f"    {attr_name}: {list(attr_value.shape)} ({attr_value.dtype})")
                            elif isinstance(attr_value, np.ndarray):
                                print(f"    {attr_name}: {list(attr_value.shape)} ({attr_value.dtype})")
                            elif attr_value is not None:
                                if hasattr(attr_value, '__len__'):
                                    print(f"    {attr_name}: {type(attr_value)} (len={len(attr_value)})")
                                else:
                                    print(f"    {attr_name}: {type(attr_value)}")
                except:
                    continue
    
    # è¾¹åˆ†æ
    print("\nğŸ”— è¾¹ç±»å‹åˆ†æ:")
    if hasattr(data, 'edge_types'):
        for edge_type in data.edge_types:
            edge_data = data[edge_type]
            print(f"  {edge_type}:")
            for attr_name in dir(edge_data):
                if not attr_name.startswith('_') and attr_name not in ['keys', 'items', 'values']:
                    try:
                        attr_value = getattr(edge_data, attr_name)
                        if torch.is_tensor(attr_value):
                            print(f"    {attr_name}: {list(attr_value.shape)} ({attr_value.dtype})")
                        elif isinstance(attr_value, np.ndarray):
                            print(f"    {attr_name}: {list(attr_value.shape)} ({attr_value.dtype})")
                        elif isinstance(attr_value, dict):
                            print(f"    {attr_name}: dict with keys {list(attr_value.keys())}")
                            # ç‰¹åˆ«å¤„ç†historyæ•°æ®
                            if attr_name == 'history':
                                analyze_history_data(attr_value, dataset_name)
                        elif attr_value is not None and not callable(attr_value):
                            if hasattr(attr_value, '__len__'):
                                print(f"    {attr_name}: {type(attr_value)} (len={len(attr_value)})")
                            else:
                                print(f"    {attr_name}: {type(attr_value)}")
                    except:
                        continue
    else:
        # å°è¯•æ£€æŸ¥å¸¸è§çš„è¾¹ç±»å‹
        common_edges = [('user', 'rated', 'item'), ('user', 'rates', 'item')]
        for edge_type in common_edges:
            try:
                edge_data = data[edge_type]
                print(f"  {edge_type}: æ‰¾åˆ°è¾¹æ•°æ®")
                if hasattr(edge_data, '__dict__'):
                    for attr_name, attr_value in edge_data.__dict__.items():
                        if torch.is_tensor(attr_value):
                            print(f"    {attr_name}: {list(attr_value.shape)} ({attr_value.dtype})")
                        elif isinstance(attr_value, dict):
                            print(f"    {attr_name}: dict with keys {list(attr_value.keys())}")
                            if attr_name == 'history':
                                analyze_history_data(attr_value, dataset_name)
                        elif attr_value is not None:
                            if hasattr(attr_value, '__len__'):
                                print(f"    {attr_name}: {type(attr_value)} (len={len(attr_value)})")
                            else:
                                print(f"    {attr_name}: {type(attr_value)}")
            except:
                continue

def analyze_history_data(history_data: dict, dataset_name: str) -> None:
    """åˆ†æhistoryæ•°æ®çš„è¯¦ç»†ç»“æ„"""
    print(f"\n    ğŸ“š {dataset_name} Historyæ•°æ®è¯¦ç»†åˆ†æ:")
    for split_name, split_data in history_data.items():
        print(f"      {split_name}:")
        if hasattr(split_data, 'items'):
            for key, value in split_data.items():
                if torch.is_tensor(value):
                    print(f"        {key}: {list(value.shape)} ({value.dtype})")
                    if value.numel() <= 10:
                        print(f"          ç¤ºä¾‹æ•°æ®: {value.tolist()}")
                elif hasattr(value, '__len__'):
                    print(f"        {key}: {type(value)} (len={len(value)})")
                else:
                    print(f"        {key}: {type(value)}")
        else:
            print(f"        ç±»å‹: {type(split_data)}")
            if hasattr(split_data, 'shape'):
                print(f"        å½¢çŠ¶: {split_data.shape}")

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ•°æ®åˆ†æ"""
    analyzer = DatasetShapeAnalyzer()
    
    # æ•°æ®è·¯å¾„é…ç½® - è¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹
    ml32m_path = "dataset/ml-32m/processed/data.pt"  # ML32Må¤„ç†åçš„æ•°æ®è·¯å¾„
    amazon_path = "dataset/amazon/processed/data_beauty.pt"  # Amazonå¤„ç†åçš„æ•°æ®è·¯å¾„
    
    print("ğŸš€ å¼€å§‹æ•°æ®é›†Shapeåˆ†æ")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    ml32m_data = analyze_ml32m_data(ml32m_path)
    amazon_data = analyze_amazon_data(amazon_path)
    
    # å¦‚æœä¸¤ä¸ªæ•°æ®é›†éƒ½æˆåŠŸåŠ è½½ï¼Œè¿›è¡Œå¯¹æ¯”åˆ†æ
    if ml32m_data is not None and amazon_data is not None:
        # ç®€åŒ–çš„å¿«é€Ÿå¯¹æ¯”
        print(f"\nğŸ¯ å¿«é€Ÿæ•°æ®å¯¹æ¯”:")
        print("=" * 50)
        quick_data_summary(ml32m_data, "ML32M")
        quick_data_summary(amazon_data, "Amazon")
        
        # è¯¦ç»†å¯¹æ¯”
        analyzer.compare_datasets(ml32m_data, amazon_data, "ML32M", "Amazon")
        
        # è¯¦ç»†çš„HeteroDataåˆ†æ
        if isinstance(ml32m_data, HeteroData):
            detailed_hetero_analysis(ml32m_data, "ML32M")
        
        if isinstance(amazon_data, HeteroData):
            detailed_hetero_analysis(amazon_data, "Amazon")
    
    # å•ç‹¬åˆ†æå¯ç”¨çš„æ•°æ®é›†
    elif ml32m_data is not None:
        print("\nğŸ“Š ML32Må•ç‹¬åˆ†æ:")
        quick_data_summary(ml32m_data, "ML32M")
        analysis = analyzer.analyze_tensor_shape(ml32m_data, "ML32M")
        analyzer.print_analysis(analysis)
        if isinstance(ml32m_data, HeteroData):
            detailed_hetero_analysis(ml32m_data, "ML32M")
    
    elif amazon_data is not None:
        print("\nğŸ“Š Amazonå•ç‹¬åˆ†æ:")
        quick_data_summary(amazon_data, "Amazon")
        analysis = analyzer.analyze_tensor_shape(amazon_data, "Amazon")
        analyzer.print_analysis(analysis)
        if isinstance(amazon_data, HeteroData):
            detailed_hetero_analysis(amazon_data, "Amazon")
    
    else:
        print("âŒ æ— æ³•åŠ è½½ä»»ä½•æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")
        print("ğŸ’¡ æç¤ºï¼šè¯·ä¿®æ”¹main()å‡½æ•°ä¸­çš„ml32m_pathå’Œamazon_pathå˜é‡")

def quick_data_summary(data: Any, dataset_name: str) -> None:
    """å¿«é€Ÿæ•°æ®æ‘˜è¦ï¼Œæ˜¾ç¤ºå…³é”®ä¿¡æ¯"""
    print(f"\nğŸ“‹ {dataset_name} æ•°æ®æ‘˜è¦:")
    print("-" * 30)
    
    if isinstance(data, HeteroData):
        # HeteroDataæ ¼å¼
        print(f"  æ ¼å¼: HeteroData")
        if hasattr(data, 'node_types'):
            print(f"  èŠ‚ç‚¹ç±»å‹: {data.node_types}")
        if hasattr(data, 'edge_types'):
            print(f"  è¾¹ç±»å‹: {len(data.edge_types)} ç§")
        
        # æ£€æŸ¥å…³é”®æ•°æ®
        try:
            if 'item' in str(data.node_types):
                item_data = data['item']
                if hasattr(item_data, 'x') and torch.is_tensor(item_data.x):
                    print(f"  ç‰©å“ç‰¹å¾: {list(item_data.x.shape)}")
                if hasattr(item_data, 'text'):
                    print(f"  ç‰©å“æ–‡æœ¬: {len(item_data.text) if hasattr(item_data.text, '__len__') else 'N/A'}")
            
            # æ£€æŸ¥è¾¹æ•°æ®
            edge_keys = [('user', 'rated', 'item'), ('user', 'rates', 'item')]
            for edge_key in edge_keys:
                try:
                    edge_data = data[edge_key]
                    if hasattr(edge_data, 'history'):
                        history = edge_data.history
                        print(f"  å†å²æ•°æ®splits: {list(history.keys())}")
                        if 'train' in history:
                            train_data = history['train']
                            if hasattr(train_data, 'items'):
                                print(f"  è®­ç»ƒæ•°æ®å­—æ®µ: {list(train_data.keys())}")
                    break
                except:
                    continue
                    
        except Exception as e:
            print(f"  âš ï¸  æ•°æ®æ£€æŸ¥å‡ºé”™: {e}")
            
    else:
        print(f"  æ ¼å¼: {type(data)}")
        if hasattr(data, '__len__'):
            print(f"  é•¿åº¦: {len(data)}")
        if hasattr(data, 'shape'):
            print(f"  å½¢çŠ¶: {data.shape}")

# å¿«é€Ÿæµ‹è¯•å‡½æ•°
def quick_shape_check(data_path: str, dataset_name: str):
    """å¿«é€Ÿæ£€æŸ¥å•ä¸ªæ•°æ®é›†çš„shape"""
    print(f"ğŸ” å¿«é€Ÿæ£€æŸ¥ {dataset_name} æ•°æ®shape:")
    print("-" * 40)
    
    try:
        data = safe_torch_load(data_path)
        if data is None:
            return
            
        analyzer = DatasetShapeAnalyzer()
        analysis = analyzer.analyze_tensor_shape(data, dataset_name, max_depth=2)
        analyzer.print_analysis(analysis)
        
        if isinstance(data, HeteroData):
            print(f"\nğŸ“‹ {dataset_name} åŸºæœ¬ä¿¡æ¯:")
            print(f"  èŠ‚ç‚¹ç±»å‹: {data.node_types}")
            print(f"  è¾¹ç±»å‹: {[str(et) for et in data.edge_types]}")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
    
    # å¦‚æœéœ€è¦å¿«é€Ÿæ£€æŸ¥å•ä¸ªæ•°æ®é›†ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # quick_shape_check("path/to/your/ml32m/data.pt", "ML32M")
    # quick_shape_check("path/to/your/amazon/data.pt", "Amazon")