#!/usr/bin/env python3
"""
æ•°æ®é›†å½¢çŠ¶æ£€æŸ¥å·¥å…·
ç”¨äºæ£€æŸ¥Amazonå’ŒMLæ•°æ®é›†çš„æ‰€æœ‰æ•°æ®å½¢çŠ¶å’Œç»Ÿè®¡ä¿¡æ¯
"""

import os
import sys
import torch
import numpy as np
from collections import defaultdict
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data.processed import ItemData, SeqData, RecDataset
from data.schemas import SeqBatch
from torch.utils.data import DataLoader


def print_separator(title: str, char: str = "=", width: int = 80):
    """æ‰“å°åˆ†éš”çº¿"""
    print(f"\n{char * width}")
    print(f"{title:^{width}}")
    print(f"{char * width}")


def print_subsection(title: str, char: str = "-", width: int = 60):
    """æ‰“å°å­æ ‡é¢˜"""
    print(f"\n{char * width}")
    print(f" {title}")
    print(f"{char * width}")


def examine_tensor_stats(tensor: torch.Tensor, name: str) -> Dict[str, Any]:
    """æ£€æŸ¥å¼ é‡çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'name': name,
        'shape': tuple(tensor.shape),
        'dtype': str(tensor.dtype),
        'device': str(tensor.device),
        'numel': tensor.numel(),
        'memory_mb': tensor.numel() * tensor.element_size() / 1024 / 1024,
    }
    
    if tensor.dtype in [torch.float32, torch.float64, torch.int32, torch.int64]:
        try:
            stats.update({
                'min': tensor.min().item(),
                'max': tensor.max().item(),
                'mean': tensor.float().mean().item(),
                'std': tensor.float().std().item(),
            })
            
            # æ£€æŸ¥ç‰¹æ®Šå€¼
            if tensor.dtype in [torch.int32, torch.int64]:
                unique_vals = torch.unique(tensor)
                stats['unique_count'] = len(unique_vals)
                stats['has_negative_one'] = (-1 in unique_vals).item() if len(unique_vals) > 0 else False
                
                # è®¡ç®—å¡«å……æ¯”ä¾‹ï¼ˆå‡è®¾-1æ˜¯å¡«å……å€¼ï¼‰
                if tensor.numel() > 0:
                    padding_count = (tensor == -1).sum().item()
                    stats['padding_ratio'] = padding_count / tensor.numel()
                else:
                    stats['padding_ratio'] = 0.0
            
        except Exception as e:
            stats['stats_error'] = str(e)
    
    return stats


def examine_item_dataset(dataset_name: str, dataset_type: RecDataset, 
                        dataset_folder: str, split: str = None) -> Dict[str, Any]:
    """æ£€æŸ¥ItemDataæ•°æ®é›†"""
    print_subsection(f"ItemData - {dataset_name}")
    
    try:
        # åŠ è½½æ•°æ®é›†
        if split:
            item_dataset = ItemData(
                root=dataset_folder,
                dataset=dataset_type,
                force_process=False,
                split=split
            )
        else:
            item_dataset = ItemData(
                root=dataset_folder,
                dataset=dataset_type,
                force_process=False
            )
        
        print(f"æ•°æ®é›†é•¿åº¦: {len(item_dataset)}")
        
        # æ£€æŸ¥å•ä¸ªæ ·æœ¬
        sample = item_dataset[0]
        print(f"æ ·æœ¬ç±»å‹: {type(sample)}")
        
        stats = {}
        for field_name in sample._fields:
            field_value = getattr(sample, field_name)
            if isinstance(field_value, torch.Tensor):
                field_stats = examine_tensor_stats(field_value, field_name)
                stats[field_name] = field_stats
                
                print(f"  {field_name}:")
                print(f"    å½¢çŠ¶: {field_stats['shape']}")
                print(f"    æ•°æ®ç±»å‹: {field_stats['dtype']}")
                print(f"    å†…å­˜: {field_stats['memory_mb']:.2f} MB")
                if 'min' in field_stats:
                    print(f"    å€¼èŒƒå›´: [{field_stats['min']}, {field_stats['max']}]")
                if 'unique_count' in field_stats:
                    print(f"    å”¯ä¸€å€¼æ•°é‡: {field_stats['unique_count']}")
        
        # æ£€æŸ¥batch
        dataloader = DataLoader(item_dataset, batch_size=4, shuffle=False)
        batch = next(iter(dataloader))
        
        print(f"\n  æ‰¹æ¬¡å½¢çŠ¶ (batch_size=4):")
        for field_name in batch._fields:
            field_value = getattr(batch, field_name)
            if isinstance(field_value, torch.Tensor):
                print(f"    {field_name}: {field_value.shape}")
        
        return {
            'dataset_length': len(item_dataset),
            'sample_stats': stats,
            'batch_shapes': {field: getattr(batch, field).shape for field in batch._fields 
                           if isinstance(getattr(batch, field), torch.Tensor)}
        }
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}


def examine_seq_dataset(dataset_name: str, dataset_type: RecDataset, 
                       dataset_folder: str, is_train: bool, split: str = None) -> Dict[str, Any]:
    """æ£€æŸ¥SeqDataæ•°æ®é›†"""
    split_name = "è®­ç»ƒ" if is_train else "æµ‹è¯•"
    print_subsection(f"SeqData - {dataset_name} ({split_name})")
    
    try:
        # åŠ è½½æ•°æ®é›†
        if split:
            seq_dataset = SeqData(
                root=dataset_folder,
                dataset=dataset_type,
                is_train=is_train,
                subsample=False,
                force_process=False,
                split=split
            )
        else:
            seq_dataset = SeqData(
                root=dataset_folder,
                dataset=dataset_type,
                is_train=is_train,
                subsample=False,
                force_process=False
            )
        
        print(f"æ•°æ®é›†é•¿åº¦: {len(seq_dataset)}")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦: {seq_dataset.max_seq_len}")
        
        # æ£€æŸ¥å•ä¸ªæ ·æœ¬
        sample = seq_dataset[0]
        print(f"æ ·æœ¬ç±»å‹: {type(sample)}")
        
        stats = {}
        for field_name in sample._fields:
            field_value = getattr(sample, field_name)
            if isinstance(field_value, torch.Tensor):
                field_stats = examine_tensor_stats(field_value, field_name)
                stats[field_name] = field_stats
                
                print(f"  {field_name}:")
                print(f"    å½¢çŠ¶: {field_stats['shape']}")
                print(f"    æ•°æ®ç±»å‹: {field_stats['dtype']}")
                print(f"    å†…å­˜: {field_stats['memory_mb']:.2f} MB")
                if 'min' in field_stats:
                    print(f"    å€¼èŒƒå›´: [{field_stats['min']}, {field_stats['max']}]")
                if 'padding_ratio' in field_stats:
                    print(f"    å¡«å……æ¯”ä¾‹: {field_stats['padding_ratio']:.2%}")
        
        # æ£€æŸ¥å¤šä¸ªæ ·æœ¬çš„åºåˆ—é•¿åº¦åˆ†å¸ƒ
        print(f"\n  åºåˆ—é•¿åº¦ç»Ÿè®¡ (æ£€æŸ¥å‰100ä¸ªæ ·æœ¬):")
        seq_lengths = []
        for i in range(min(100, len(seq_dataset))):
            sample = seq_dataset[i]
            if hasattr(sample, 'seq_mask'):
                seq_len = sample.seq_mask.sum().item()
                seq_lengths.append(seq_len)
        
        if seq_lengths:
            seq_lengths = np.array(seq_lengths)
            print(f"    å¹³å‡é•¿åº¦: {seq_lengths.mean():.2f}")
            print(f"    é•¿åº¦èŒƒå›´: [{seq_lengths.min()}, {seq_lengths.max()}]")
            print(f"    æ ‡å‡†å·®: {seq_lengths.std():.2f}")
        
        # æ£€æŸ¥batch
        dataloader = DataLoader(seq_dataset, batch_size=4, shuffle=False)
        batch = next(iter(dataloader))
        
        print(f"\n  æ‰¹æ¬¡å½¢çŠ¶ (batch_size=4):")
        for field_name in batch._fields:
            field_value = getattr(batch, field_name)
            if isinstance(field_value, torch.Tensor):
                print(f"    {field_name}: {field_value.shape}")
        
        return {
            'dataset_length': len(seq_dataset),
            'max_seq_len': seq_dataset.max_seq_len,
            'sample_stats': stats,
            'seq_length_stats': {
                'mean': seq_lengths.mean() if len(seq_lengths) > 0 else 0,
                'min': seq_lengths.min() if len(seq_lengths) > 0 else 0,
                'max': seq_lengths.max() if len(seq_lengths) > 0 else 0,
                'std': seq_lengths.std() if len(seq_lengths) > 0 else 0,
            },
            'batch_shapes': {field: getattr(batch, field).shape for field in batch._fields 
                           if isinstance(getattr(batch, field), torch.Tensor)}
        }
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}


def examine_raw_data_files(dataset_name: str, dataset_type: RecDataset, 
                          dataset_folder: str, split: str = None) -> Dict[str, Any]:
    """æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶"""
    print_subsection(f"åŸå§‹æ•°æ®æ–‡ä»¶ - {dataset_name}")
    
    try:
        # åŠ è½½åŸå§‹æ•°æ®é›†æ¥æ£€æŸ¥å¤„ç†åçš„HeteroData
        if dataset_type == RecDataset.AMAZON:
            from data.amazon import AmazonReviews
            if split:
                raw_dataset = AmazonReviews(root=dataset_folder, split=split)
            else:
                raw_dataset = AmazonReviews(root=dataset_folder, split="beauty")
        elif dataset_type == RecDataset.ML_32M:
            from data.ml32m import RawMovieLens32M
            raw_dataset = RawMovieLens32M(root=dataset_folder)
        else:  # ML_1M
            from data.ml1m import RawMovieLens1M
            raw_dataset = RawMovieLens1M(root=dataset_folder)
        
        data = raw_dataset.data
        
        print(f"HeteroDataèŠ‚ç‚¹ç±»å‹: {list(data.node_types)}")
        print(f"HeteroDataè¾¹ç±»å‹: {list(data.edge_types)}")
        
        # æ£€æŸ¥itemèŠ‚ç‚¹æ•°æ®
        if 'item' in data:
            print(f"\n  itemèŠ‚ç‚¹æ•°æ®:")
            for key, value in data['item'].items():
                if isinstance(value, torch.Tensor):
                    print(f"    {key}: {value.shape}, {value.dtype}")
                elif isinstance(value, np.ndarray):
                    print(f"    {key}: {value.shape}, {value.dtype}")
                else:
                    print(f"    {key}: {type(value)}")
        
        # æ£€æŸ¥è¾¹æ•°æ®
        edge_key = ("user", "rated", "item")
        if edge_key in data:
            print(f"\n  {edge_key}è¾¹æ•°æ®:")
            edge_data = data[edge_key]
            for key, value in edge_data.items():
                if isinstance(value, dict):
                    print(f"    {key}:")
                    for sub_key, sub_value in value.items():
                        if isinstance(sub_value, torch.Tensor):
                            print(f"      {sub_key}: {sub_value.shape}, {sub_value.dtype}")
                        else:
                            print(f"      {sub_key}: {type(sub_value)}")
        
        return {'success': True}
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}


def examine_dataset(dataset_name: str, dataset_type: RecDataset, 
                   dataset_folder: str, split: str = None) -> Dict[str, Any]:
    """å®Œæ•´æ£€æŸ¥ä¸€ä¸ªæ•°æ®é›†"""
    print_separator(f"æ£€æŸ¥æ•°æ®é›†: {dataset_name}")
    
    results = {}
    
    # 1. æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶
    results['raw_data'] = examine_raw_data_files(dataset_name, dataset_type, dataset_folder, split)
    
    # 2. æ£€æŸ¥ItemData
    results['item_data'] = examine_item_dataset(dataset_name, dataset_type, dataset_folder, split)
    
    # 3. æ£€æŸ¥SeqData (è®­ç»ƒé›†)
    results['seq_data_train'] = examine_seq_dataset(dataset_name, dataset_type, dataset_folder, True, split)
    
    # 4. æ£€æŸ¥SeqData (æµ‹è¯•é›†)
    results['seq_data_test'] = examine_seq_dataset(dataset_name, dataset_type, dataset_folder, False, split)
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    print_separator("æ•°æ®é›†å½¢çŠ¶æ£€æŸ¥å·¥å…·", "=", 80)
    
    # æ•°æ®é›†é…ç½®
    datasets_config = [
        {
            'name': 'Amazon Beauty',
            'type': RecDataset.AMAZON,
            'folder': 'dataset/amazon',
            'split': 'beauty'
        },
        {
            'name': 'MovieLens 32M',
            'type': RecDataset.ML_32M,
            'folder': 'dataset/ml-32m',
            'split': None
        },
        {
            'name': 'MovieLens 1M',
            'type': RecDataset.ML_1M,
            'folder': 'dataset/ml-1m',
            'split': None
        }
    ]
    
    all_results = {}
    
    for config in datasets_config:
        try:
            print(f"\næ­£åœ¨æ£€æŸ¥ {config['name']}...")
            results = examine_dataset(
                config['name'], 
                config['type'], 
                config['folder'], 
                config['split']
            )
            all_results[config['name']] = results
            
        except Exception as e:
            print(f"æ£€æŸ¥ {config['name']} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            all_results[config['name']] = {'error': str(e)}
    
    # æ€»ç»“æŠ¥å‘Š
    print_separator("æ€»ç»“æŠ¥å‘Š")
    
    for dataset_name, results in all_results.items():
        print(f"\n{dataset_name}:")
        
        if 'error' in results:
            print(f"  âŒ æ£€æŸ¥å¤±è´¥: {results['error']}")
            continue
        
        # ItemDataæ€»ç»“
        if 'item_data' in results and 'error' not in results['item_data']:
            item_data = results['item_data']
            print(f"  ğŸ“¦ ItemData: {item_data['dataset_length']} ä¸ªç‰©å“")
        
        # SeqDataæ€»ç»“
        if 'seq_data_train' in results and 'error' not in results['seq_data_train']:
            train_data = results['seq_data_train']
            print(f"  ğŸš‚ è®­ç»ƒåºåˆ—: {train_data['dataset_length']} ä¸ª")
            print(f"     æœ€å¤§åºåˆ—é•¿åº¦: {train_data['max_seq_len']}")
            
        if 'seq_data_test' in results and 'error' not in results['seq_data_test']:
            test_data = results['seq_data_test']
            print(f"  ğŸ§ª æµ‹è¯•åºåˆ—: {test_data['dataset_length']} ä¸ª")
    
    print_separator("æ£€æŸ¥å®Œæˆ")


if __name__ == "__main__":
    main()
