import argparse
import os
import gin
import torch
import wandb

from accelerate import Accelerator
from data.processed import ItemData
from data.processed import RecDataset
from data.processed import SeqData
from data.utils import batch_to
from data.utils import cycle
from data.utils import next_batch
from evaluate.metrics import TopKAccumulator
from modules.model import EncoderDecoderRetrievalModel
from modules.scheduler.inv_sqrt import InverseSquareRootScheduler
from modules.tokenizer.semids import SemanticIdTokenizer
from modules.utils import compute_debug_metrics
from modules.utils import parse_config
from huggingface_hub import login
from torch.optim import AdamW
from torch.utils.data import BatchSampler
from torch.utils.data import DataLoader
from torch.utils.data import RandomSampler
from tqdm import tqdm

def debug_generation_step_by_step(model, tokenized_data):
    """ä¸€æ­¥æ­¥è°ƒè¯•generationè¿‡ç¨‹"""
    print(f"\nğŸ§ª å¼€å§‹é€æ­¥è°ƒè¯•generation...")
    
    model.eval()
    model.enable_generation = True
    
    # æµ‹è¯•åŸºæœ¬forward
    print(f"1. æµ‹è¯•åŸºæœ¬forward...")
    try:
        with torch.no_grad():
            basic_output = model(tokenized_data)
            if basic_output.loss is not None:
                print(f"   âœ… åŸºæœ¬forwardæˆåŠŸ, loss: {basic_output.loss:.4f}")
            else:
                print(f"   âœ… åŸºæœ¬forwardæˆåŠŸ, loss: None (evaluationæ¨¡å¼)")
    except Exception as e:
        print(f"   âŒ åŸºæœ¬forwardå¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ç®€åŒ–çš„generation
    print(f"2. æµ‹è¯•ç®€åŒ–generation...")
    try:
        # åªåšç¬¬ä¸€æ­¥generation
        input_batch = TokenizedSeqBatch(
            user_ids=tokenized_data.user_ids,
            sem_ids=tokenized_data.sem_ids,
            sem_ids_fut=None,
            seq_mask=tokenized_data.seq_mask,
            token_type_ids=tokenized_data.token_type_ids,
            token_type_ids_fut=None
        )
        
        with torch.no_grad():
            first_step_output = model.forward(input_batch)
            print(f"   âœ… ç¬¬ä¸€æ­¥generation forwardæˆåŠŸ")
            
            # æµ‹è¯•sampling
            logits = first_step_output.logits
            if logits is None:
                print(f"   âŒ logitsæ˜¯None")
                return False
                
            probas = F.softmax(logits, dim=-1)
            samples = torch.multinomial(probas, num_samples=10)
            print(f"   âœ… samplingæˆåŠŸ, samples range: [{samples.min()}, {samples.max()}]")
            
            # æ£€æŸ¥é‡‡æ ·ç»“æœæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if samples.max() >= model.num_embeddings:
                print(f"   âŒ é‡‡æ ·ç»“æœè¶Šç•Œ: {samples.max()} >= {model.num_embeddings}")
                return False
            
            # æµ‹è¯•ç¬¬äºŒæ­¥
            next_sem_ids = samples[:, 0:1]  # åªå–ç¬¬ä¸€ä¸ªsample
            print(f"   å‡†å¤‡ç¬¬äºŒæ­¥: next_sem_ids = {next_sem_ids.flatten()[:5].tolist()}...")
            
            input_batch_step2 = TokenizedSeqBatch(
                user_ids=tokenized_data.user_ids,
                sem_ids=tokenized_data.sem_ids,
                sem_ids_fut=next_sem_ids,
                seq_mask=tokenized_data.seq_mask,
                token_type_ids=tokenized_data.token_type_ids,
                token_type_ids_fut=torch.zeros_like(next_sem_ids)
            )
            
            second_step_output = model.forward(input_batch_step2)
            print(f"   âœ… ç¬¬äºŒæ­¥generation forwardæˆåŠŸ")
            
    except Exception as e:
        print(f"   âŒ ç®€åŒ–generationå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print(f"3. åŸºç¡€æµ‹è¯•éƒ½é€šè¿‡ï¼Œé—®é¢˜å¯èƒ½åœ¨å¤æ‚çš„beam searché€»è¾‘ä¸­")
    return True


def analyze_amazon_dataset_info(item_dataset, train_dataset, eval_dataset):
    """åˆ†æAmazonæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯"""
    print(f"\nğŸ“Š Amazonæ•°æ®é›†è¯¦ç»†åˆ†æ:")
    
    # ç‰©å“æ•°æ®é›†ä¿¡æ¯
    print(f"ğŸ·ï¸  ç‰©å“æ•°æ®é›†ä¿¡æ¯:")
    print(f"  ç‰©å“æ€»æ•°: {len(item_dataset):,}")
    print(f"  ç‰©å“ç‰¹å¾ç»´åº¦: {item_dataset.item_data.shape}")
    
    # è®­ç»ƒæ•°æ®é›†ä¿¡æ¯
    print(f"ğŸš‚ è®­ç»ƒæ•°æ®é›†ä¿¡æ¯:")
    print(f"  è®­ç»ƒåºåˆ—æ•°: {len(train_dataset):,}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {train_dataset.max_seq_len}")
    
    # æŠ½æ ·æ£€æŸ¥å‡ ä¸ªè®­ç»ƒæ ·æœ¬
    sample_indices = [0, len(train_dataset)//2, len(train_dataset)-1]
    print(f"  è®­ç»ƒæ ·æœ¬æ£€æŸ¥:")
    for i in sample_indices:
        sample = train_dataset[i]
        print(f"    æ ·æœ¬{i}: user_ids={sample.user_ids.shape}, ids={sample.ids.shape}, seq_mask_sum={sample.seq_mask.sum()}")
    
    # è¯„ä¼°æ•°æ®é›†ä¿¡æ¯
    print(f"ğŸ” è¯„ä¼°æ•°æ®é›†ä¿¡æ¯:")
    print(f"  è¯„ä¼°åºåˆ—æ•°: {len(eval_dataset):,}")
    
    # æŠ½æ ·æ£€æŸ¥å‡ ä¸ªè¯„ä¼°æ ·æœ¬
    print(f"  è¯„ä¼°æ ·æœ¬æ£€æŸ¥:")
    for i in sample_indices[:2]:  # åªæ£€æŸ¥å‰ä¸¤ä¸ª
        if i < len(eval_dataset):
            sample = eval_dataset[i]
            print(f"    æ ·æœ¬{i}: user_ids={sample.user_ids.shape}, ids={sample.ids.shape}, seq_mask_sum={sample.seq_mask.sum()}")


@gin.configurable
def train(
    iterations=500000,
    batch_size=64,
    learning_rate=0.001,
    weight_decay=0.01,
    dataset_folder="dataset/ml-1m",
    save_dir_root="out/",
    dataset=RecDataset.ML_1M,
    pretrained_rqvae_path=None,
    pretrained_decoder_path=None,
    split_batches=True,
    amp=False,
    wandb_logging=False,
    force_dataset_process=False,
    mixed_precision_type="fp16",
    gradient_accumulate_every=1,
    save_model_every=1000000,
    partial_eval_every=1000,
    full_eval_every=10000,
    vae_input_dim=18,
    vae_embed_dim=16,
    vae_hidden_dims=[18, 18],
    vae_codebook_size=32,
    vae_codebook_normalize=False,
    vae_sim_vq=False,
    vae_n_cat_feats=18,
    vae_n_layers=3,
    decoder_embed_dim=64,
    dropout_p=0.1,
    attn_heads=8,
    attn_embed_dim=64,
    attn_layers=4,
    dataset_split="beauty",
    push_vae_to_hf=False,
    train_data_subsample=True,
    model_jagged_mode=True,
    vae_hf_model_name="edobotta/rqvae-amazon-beauty"
):  
    if dataset != RecDataset.AMAZON:
        raise Exception(f"Dataset currently not supported: {dataset}.")

    if wandb_logging:
        params = locals()

    accelerator = Accelerator(
        split_batches=split_batches,
        mixed_precision=mixed_precision_type if amp else 'no'
    )

    device = accelerator.device

    if wandb_logging and accelerator.is_main_process:
        wandb.login()
        run = wandb.init(
            project="gen-retrieval-decoder-training",
            config=params
        )
    
    item_dataset = ItemData(
        root=dataset_folder,
        dataset=dataset,
        force_process=force_dataset_process,
        split=dataset_split
    )
    train_dataset = SeqData(
        root=dataset_folder, 
        dataset=dataset, 
        is_train=True, 
        subsample=train_data_subsample, 
        split=dataset_split
    )
    eval_dataset = SeqData(
        root=dataset_folder, 
        dataset=dataset, 
        is_train=False, 
        subsample=False, 
        split=dataset_split
    )

    print(f"\n" + "="*60)
    print("Amazonæ•°æ®é›†ä¿¡æ¯åˆ†æ")
    print("="*60)
    analyze_amazon_dataset_info(item_dataset, train_dataset, eval_dataset)
    print("="*60)

    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    train_dataloader = cycle(train_dataloader)
    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)
    
    train_dataloader, eval_dataloader = accelerator.prepare(
        train_dataloader, eval_dataloader
    )

    tokenizer = SemanticIdTokenizer(
        input_dim=vae_input_dim,
        hidden_dims=vae_hidden_dims,
        output_dim=vae_embed_dim,
        codebook_size=vae_codebook_size,
        n_layers=vae_n_layers,
        n_cat_feats=vae_n_cat_feats,
        rqvae_weights_path=pretrained_rqvae_path,
        rqvae_codebook_normalize=vae_codebook_normalize,
        rqvae_sim_vq=vae_sim_vq
    )
    tokenizer = accelerator.prepare(tokenizer)
    tokenizer.precompute_corpus_ids(item_dataset)

    print(f"\nğŸ“Š Amazoné¢„è®¡ç®—corpusæ£€æŸ¥:")
    print(f"corpuså½¢çŠ¶: {tokenizer.cached_ids.shape}")
    print(f"corpusè¯­ä¹‰IDèŒƒå›´: [{tokenizer.cached_ids[:, :-1].min()}, {tokenizer.cached_ids[:, :-1].max()}]")
    print(f"å»é‡ç»´åº¦èŒƒå›´: [{tokenizer.cached_ids[:, -1].min()}, {tokenizer.cached_ids[:, -1].max()}]")

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºcodebook_sizeçš„å€¼
    for i in range(tokenizer.cached_ids.shape[1] - 1):  # æ£€æŸ¥å‰3å±‚
        layer_data = tokenizer.cached_ids[:, i]
        layer_max = layer_data.max().item()
        layer_min = layer_data.min().item()
        print(f"ç¬¬{i}å±‚è¯­ä¹‰IDèŒƒå›´: [{layer_min}, {layer_max}]")
        if layer_max >= vae_codebook_size:
            print(f"âŒ ç¬¬{i}å±‚è¯­ä¹‰IDè¶…å‡ºèŒƒå›´: max={layer_max} >= codebook_size={vae_codebook_size}")
            over_count = (layer_data >= vae_codebook_size).sum().item()
            print(f"   è¶…å‡ºæ•°é‡: {over_count}/{layer_data.numel()}")


    
    if push_vae_to_hf:
        login()
        tokenizer.rq_vae.push_to_hub(vae_hf_model_name)

    model = EncoderDecoderRetrievalModel(
        embedding_dim=decoder_embed_dim,
        attn_dim=attn_embed_dim,
        dropout=dropout_p,
        num_heads=attn_heads,
        n_layers=attn_layers,
        num_embeddings=vae_codebook_size,
        inference_verifier_fn=lambda x: tokenizer.exists_prefix(x),
        sem_id_dim=tokenizer.sem_ids_dim,
        max_pos=train_dataset.max_seq_len*tokenizer.sem_ids_dim,
        jagged_mode=model_jagged_mode
    )

    optimizer = AdamW(
        params=model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay
    )

    lr_scheduler = InverseSquareRootScheduler(
        optimizer=optimizer,
        warmup_steps=10000
    )
    
    start_iter = 0
    if pretrained_decoder_path is not None:
        checkpoint = torch.load(pretrained_decoder_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint["model"])
        optimizer.load_state_dict(checkpoint["optimizer"])
        if "scheduler" in checkpoint:
            lr_scheduler.load_state_dict(checkpoint["scheduler"])
        start_iter = checkpoint["iter"] + 1

    model, optimizer, lr_scheduler = accelerator.prepare(
        model, optimizer, lr_scheduler
    )

    metrics_accumulator = TopKAccumulator(ks=[1, 5, 10])
    num_params = sum(p.numel() for p in model.parameters())
    print(f"Device: {device}, Num Parameters: {num_params}")
    with tqdm(initial=start_iter, total=start_iter + iterations,
              disable=not accelerator.is_main_process) as pbar:
        for iter in range(iterations):
            model.train()
            total_loss = 0
            optimizer.zero_grad()
            for _ in range(gradient_accumulate_every):
                data = next_batch(train_dataloader, device)
                tokenized_data = tokenizer(data)

                with accelerator.autocast():
                    model_output = model(tokenized_data)
                    loss = model_output.loss / gradient_accumulate_every
                    total_loss += loss
                
                if wandb_logging and accelerator.is_main_process:
                    train_debug_metrics = compute_debug_metrics(tokenized_data, model_output)

                accelerator.backward(total_loss)
                assert model.sem_id_embedder.emb.weight.grad is not None

            pbar.set_description(f'loss: {total_loss.item():.4f}')

            accelerator.wait_for_everyone()

            optimizer.step()
            lr_scheduler.step()

            accelerator.wait_for_everyone()

            if (iter+1) % partial_eval_every == 0:
                model.eval()
                model.enable_generation = False
                for batch in eval_dataloader:
                    data = batch_to(batch, device)
                    tokenized_data = tokenizer(data)

                    with torch.no_grad():
                        model_output_eval = model(tokenized_data)

                    if wandb_logging and accelerator.is_main_process:
                        eval_debug_metrics = compute_debug_metrics(tokenized_data, model_output_eval, "eval")
                        eval_debug_metrics["eval_loss"] = model_output_eval.loss.detach().cpu().item()
                        wandb.log(eval_debug_metrics)

            # if (iter+1) % full_eval_every == 0:
            #     model.eval()
            #     model.enable_generation = True
            #     with tqdm(eval_dataloader, desc=f'Eval {iter+1}', disable=not accelerator.is_main_process) as pbar_eval:
            #         for batch in pbar_eval:
            #             data = batch_to(batch, device)
            #             tokenized_data = tokenizer(data)

            #             generated = model.generate_next_sem_id(tokenized_data, top_k=True, temperature=1)
            #             actual, top_k = tokenized_data.sem_ids_fut, generated.sem_ids

            #             metrics_accumulator.accumulate(actual=actual, top_k=top_k)

            #             if accelerator.is_main_process and wandb_logging:
            #                 wandb.log(eval_debug_metrics)
                
            #     eval_metrics = metrics_accumulator.reduce()
                
            #     print(eval_metrics)
            #     if accelerator.is_main_process and wandb_logging:
            #         wandb.log(eval_metrics)
                
            #     metrics_accumulator.reset()

            if (iter+1) % full_eval_every == 0:
                print(f"\nğŸ”§ å¼€å§‹Amazon generation evaluationè°ƒè¯• (iteration {iter+1})...")
                model.eval()
                
                # ç›´æ¥éå†dataloaderè·å–ç¬¬ä¸€ä¸ªbatch
                for batch_idx, batch in enumerate(eval_dataloader):
                    if batch_idx == 0:  # åªå¤„ç†ç¬¬ä¸€ä¸ªbatch
                        data = batch_to(batch, device)
                        
                        print(f"ğŸ“Š Amazonç¬¬ä¸€ä¸ªevaluation batchä¿¡æ¯:")
                        print(f"  åŸå§‹batch size: {data.user_ids.shape[0]}")
                        print(f"  åºåˆ—é•¿åº¦: {data.ids.shape[1]}")
                        print(f"  ç‰©å“IDèŒƒå›´: [{data.ids.min()}, {data.ids.max()}]")
                        print(f"  futureç‰©å“ID: [{data.ids_fut.min()}, {data.ids_fut.max()}]")
                        print(f"  åºåˆ—maskæ€»å’Œ: {data.seq_mask.sum()}")
                        
                        # åˆ›å»ºæ›´å°çš„æµ‹è¯•batchï¼ˆAmazonæ•°æ®å¯èƒ½batch sizeæ›´å¤§ï¼‰
                        test_batch_size = min(4, data.user_ids.shape[0])  # æœ€å¤š4ä¸ªæ ·æœ¬
                        small_data = SeqBatch(
                            user_ids=data.user_ids[:test_batch_size],
                            ids=data.ids[:test_batch_size],
                            ids_fut=data.ids_fut[:test_batch_size],
                            x=data.x[:test_batch_size],
                            x_fut=data.x_fut[:test_batch_size],
                            seq_mask=data.seq_mask[:test_batch_size]
                        )
                        
                        tokenized_data = tokenizer(small_data)
                        
                        print(f"ğŸ“Š Amazon tokenizedæ•°æ®ä¿¡æ¯:")
                        print(f"  æµ‹è¯•batch size: {test_batch_size}")
                        print(f"  sem_ids shape: {tokenized_data.sem_ids.shape}")
                        print(f"  sem_ids range: [{tokenized_data.sem_ids.min()}, {tokenized_data.sem_ids.max()}]")
                        print(f"  sem_ids_fut range: [{tokenized_data.sem_ids_fut.min()}, {tokenized_data.sem_ids_fut.max()}]")
                        print(f"  token_type_ids range: [{tokenized_data.token_type_ids.min()}, {tokenized_data.token_type_ids.max()}]")
                        print(f"  seq_mask sum: {tokenized_data.seq_mask.sum()}")
                        
                        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                        if tokenized_data.sem_ids.max() >= model.num_embeddings:
                            print(f"âŒ sem_idsè¶Šç•Œ: {tokenized_data.sem_ids.max()} >= {model.num_embeddings}")
                            break
                        
                        if tokenized_data.token_type_ids.max() >= model.sem_id_dim:
                            print(f"âŒ token_type_idsè¶Šç•Œ: {tokenized_data.token_type_ids.max()} >= {model.sem_id_dim}")
                            break
                        
                        # æ‰§è¡Œé€æ­¥è°ƒè¯•
                        success = debug_generation_step_by_step(model, tokenized_data)
                        
                        if success:
                            print(f"\nâœ… AmazonåŸºç¡€æµ‹è¯•é€šè¿‡ï¼Œå°è¯•å®Œæ•´generation...")
                            try:
                                model.enable_generation = True
                                # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
                                generated = model.generate_next_sem_id(tokenized_data, top_k=False, temperature=1)  # ç¦ç”¨top_k
                                print(f"ğŸ‰ Amazonå®Œæ•´generationæˆåŠŸ!")
                                
                                if generated is not None:
                                    actual, top_k = tokenized_data.sem_ids_fut, generated.sem_ids
                                    metrics_accumulator.accumulate(actual=actual, top_k=top_k)
                                    
                                    # å¦‚æœæˆåŠŸï¼Œå°è¯•æ›´å¤šbatchä½†é™åˆ¶æ•°é‡
                                    print(f"\nğŸš€ ç¬¬ä¸€ä¸ªbatchæˆåŠŸï¼Œç»§ç»­evaluationæ›´å¤šAmazon batch...")
                                    successful_batches = 1
                                    total_batches = 1
                                    
                                    continue_eval = True
                                    for eval_batch_idx, eval_batch in enumerate(eval_dataloader):
                                        if eval_batch_idx >= 9:  # æœ€å¤š10ä¸ªbatch
                                            break
                                        if eval_batch_idx == 0:  # è·³è¿‡ç¬¬ä¸€ä¸ªï¼Œå·²ç»å¤„ç†è¿‡
                                            continue
                                            
                                        try:
                                            eval_data = batch_to(eval_batch, device)
                                            # åŒæ ·é™åˆ¶batch size
                                            limited_batch_size = min(4, eval_data.user_ids.shape[0])
                                            limited_data = SeqBatch(
                                                user_ids=eval_data.user_ids[:limited_batch_size],
                                                ids=eval_data.ids[:limited_batch_size],
                                                ids_fut=eval_data.ids_fut[:limited_batch_size],
                                                x=eval_data.x[:limited_batch_size],
                                                x_fut=eval_data.x_fut[:limited_batch_size],
                                                seq_mask=eval_data.seq_mask[:limited_batch_size]
                                            )
                                            
                                            eval_tokenized = tokenizer(limited_data)
                                            
                                            # å¿«é€Ÿæ£€æŸ¥
                                            if eval_tokenized.sem_ids.max() >= model.num_embeddings:
                                                print(f"è·³è¿‡batch {eval_batch_idx+1}: sem_idsè¶Šç•Œ")
                                                continue
                                            
                                            eval_generated = model.generate_next_sem_id(eval_tokenized, top_k=False, temperature=1)
                                            
                                            if eval_generated is not None:
                                                eval_actual, eval_top_k = eval_tokenized.sem_ids_fut, eval_generated.sem_ids
                                                metrics_accumulator.accumulate(actual=eval_actual, top_k=eval_top_k)
                                                successful_batches += 1
                                            
                                            total_batches += 1
                                            
                                        except RuntimeError as e:
                                            if "illegal memory access" in str(e):
                                                print(f"âŒ Amazon batch {eval_batch_idx+1} CUDAé”™è¯¯ï¼Œåœæ­¢evaluation")
                                                break
                                            else:
                                                print(f"âŒ Amazon batch {eval_batch_idx+1} å…¶ä»–é”™è¯¯: {e}")
                                                break
                                        except Exception as e:
                                            print(f"âŒ Amazon batch {eval_batch_idx+1} æ„å¤–é”™è¯¯: {e}")
                                            continue
                                    
                                    print(f"\nAmazon Evaluationå®Œæˆ: {successful_batches}/{total_batches} æˆåŠŸ")
                                    
                                    if successful_batches > 0:
                                        eval_metrics = metrics_accumulator.reduce()
                                        print(f"Amazonè¯„ä¼°ç»“æœ: {eval_metrics}")
                                        if accelerator.is_main_process and wandb_logging:
                                            wandb.log(eval_metrics)
                                else:
                                    print(f"âš ï¸ Amazon generationè¿”å›None")
                                    
                            except RuntimeError as e:
                                if "illegal memory access" in str(e):
                                    print(f"âŒ Amazonå®Œæ•´generationå‡ºç°CUDAé”™è¯¯:")
                                    print(f"   é”™è¯¯ä¿¡æ¯: {e}")
                                    print(f"   è¿™ä¸ªé”™è¯¯éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•...")
                                else:
                                    print(f"âŒ Amazonå®Œæ•´generationå‡ºç°å…¶ä»–é”™è¯¯: {e}")
                                    raise e
                            except Exception as e:
                                print(f"âŒ Amazonå®Œæ•´generationå‡ºç°æ„å¤–é”™è¯¯: {e}")
                                import traceback
                                traceback.print_exc()
                        else:
                            print(f"âŒ AmazonåŸºç¡€æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œgeneration")
                            
                        break  # åªå¤„ç†ç¬¬ä¸€ä¸ªbatch
                
                # é‡ç½®accumulator
                metrics_accumulator.reset()

            if accelerator.is_main_process:
                if (iter+1) % save_model_every == 0 or iter+1 == iterations:
                    state = {
                        "iter": iter,
                        "model": model.state_dict(),
                        "optimizer": optimizer.state_dict(),
                        "scheduler": lr_scheduler.state_dict()
                    }

                    if not os.path.exists(save_dir_root):
                        os.makedirs(save_dir_root)

                    torch.save(state, save_dir_root + f"checkpoint_{iter}.pt")
                
                if wandb_logging:
                    wandb.log({
                        "learning_rate": optimizer.param_groups[0]["lr"],
                        "total_loss": total_loss.cpu().item(),
                        **train_debug_metrics
                    })

            pbar.update(1)
    
    if wandb_logging:
        wandb.finish()


if __name__ == "__main__":
    parse_config()
    train()
